
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.5. Stochastic Gradient Descent" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://scikit-learn/stable/modules/sgd.html" />
<meta property="og:site_name" content="scikit-learn" />
<meta property="og:description" content="Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logis..." />
<meta property="og:image" content="https://scikit-learn/stable/_images/sphx_glr_plot_sgd_separating_hyperplane_001.png" />
<meta property="og:image:alt" content="scikit-learn" />
<meta name="description" content="Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logis..." />

    <title>1.5. Stochastic Gradient Descent &#8212; scikit-learn 1.5.2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Vibur" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyterlite_sphinx.css?v=ca70e7f1" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/colors.css?v=cc94ab7d" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/custom.css?v=e4cb1417" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=73275c37"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=97f0b27d"></script>
    <script src="../_static/jupyterlite_sphinx.js?v=d6bdf5f8"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script data-domain="scikit-learn.org" defer="defer" src="https://views.scientific-python.org/js/script.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/sgd';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://scikit-learn.org/dev/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.5.2';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../_static/scripts/dropdown.js?v=e2048168"></script>
    <script src="../_static/scripts/version-switcher.js?v=a6dd8357"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.6. Nearest Neighbors" href="neighbors.html" />
    <link rel="prev" title="1.4. Support Vector Machines" href="svm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/scikit-learn-logo-small.png" class="logo__image only-light" alt="scikit-learn homepage"/>
    <script>document.write(`<img src="../_static/scikit-learn-logo-small.png" class="logo__image only-dark" alt="scikit-learn homepage"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install.html">
    Install
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/index.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://blog.scikit-learn.org/">
    Community
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../getting_started.html">
    Getting Started
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../whats_new.html">
    Release History
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../glossary.html">
    Glossary
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://scikit-learn.org/dev/developers/index.html">
    Development
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../support.html">
    Support
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../related_projects.html">
    Related Projects
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../roadmap.html">
    Roadmap
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../governance.html">
    Governance
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../about.html">
    About us
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-learn/scikit-learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install.html">
    Install
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/index.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://blog.scikit-learn.org/">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../whats_new.html">
    Release History
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../glossary.html">
    Glossary
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://scikit-learn.org/dev/developers/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../support.html">
    Support
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../related_projects.html">
    Related Projects
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../roadmap.html">
    Roadmap
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../governance.html">
    Governance
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about.html">
    About us
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-learn/scikit-learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../supervised_learning.html">1. Supervised learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear_model.html">1.1. Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lda_qda.html">1.2. Linear and Quadratic Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernel_ridge.html">1.3. Kernel ridge regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm.html">1.4. Support Vector Machines</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.5. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors.html">1.6. Nearest Neighbors</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process.html">1.7. Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="cross_decomposition.html">1.8. Cross decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive_bayes.html">1.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree.html">1.10. Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble.html">1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiclass.html">1.12. Multiclass and multioutput algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection.html">1.13. Feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised.html">1.14. Semi-supervised learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="isotonic.html">1.15. Isotonic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="calibration.html">1.16. Probability calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_networks_supervised.html">1.17. Neural network models (supervised)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unsupervised_learning.html">2. Unsupervised learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="mixture.html">2.1. Gaussian mixture models</a></li>
<li class="toctree-l2"><a class="reference internal" href="manifold.html">2.2. Manifold learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering.html">2.3. Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="biclustering.html">2.4. Biclustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition.html">2.5. Decomposing signals in components (matrix factorization problems)</a></li>
<li class="toctree-l2"><a class="reference internal" href="covariance.html">2.6. Covariance estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="outlier_detection.html">2.7. Novelty and Outlier Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="density.html">2.8. Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_networks_unsupervised.html">2.9. Neural network models (unsupervised)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../model_selection.html">3. Model selection and evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="cross_validation.html">3.1. Cross-validation: evaluating estimator performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="grid_search.html">3.2. Tuning the hyper-parameters of an estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_threshold.html">3.3. Tuning the decision threshold for class prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_evaluation.html">3.4. Metrics and scoring: quantifying the quality of predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="learning_curve.html">3.5. Validation curves: plotting scores to evaluate models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../inspection.html">4. Inspection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="partial_dependence.html">4.1. Partial Dependence and Individual Conditional Expectation plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="permutation_importance.html">4.2. Permutation feature importance</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../visualizations.html">5. Visualizations</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_transforms.html">6. Dataset transformations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="compose.html">6.1. Pipelines and composite estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_extraction.html">6.2. Feature extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing.html">6.3. Preprocessing data</a></li>
<li class="toctree-l2"><a class="reference internal" href="impute.html">6.4. Imputation of missing values</a></li>
<li class="toctree-l2"><a class="reference internal" href="unsupervised_reduction.html">6.5. Unsupervised dimensionality reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="random_projection.html">6.6. Random Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernel_approximation.html">6.7. Kernel Approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html">6.8. Pairwise metrics, Affinities and Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing_targets.html">6.9. Transforming the prediction target (<code class="docutils literal notranslate"><span class="pre">y</span></code>)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../datasets.html">7. Dataset loading utilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../datasets/toy_dataset.html">7.1. Toy datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../datasets/real_world.html">7.2. Real world datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../datasets/sample_generators.html">7.3. Generated datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../datasets/loading_other_datasets.html">7.4. Loading other datasets</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../computing.html">8. Computing with scikit-learn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../computing/scaling_strategies.html">8.1. Strategies to scale computationally: bigger data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computing/computational_performance.html">8.2. Computational Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computing/parallelism.html">8.3. Parallelism, resource management, and configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../model_persistence.html">9. Model persistence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common_pitfalls.html">10. Common pitfalls and recommended practices</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dispatching.html">11. Dispatching</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="array_api.html">11.1. Array API support (experimental)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning_map.html">12. Choosing the right estimator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../presentations.html">13. External Resources, Videos and Talks</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../user_guide.html" class="nav-link">User Guide</a></li>
    
    
    <li class="breadcrumb-item"><a href="../supervised_learning.html" class="nav-link"><span class="section-number">1. </span>Supervised learning</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="stochastic-gradient-descent">
<span id="sgd"></span><h1><span class="section-number">1.5. </span>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Link to this heading">#</a></h1>
<p><strong>Stochastic Gradient Descent (SGD)</strong> is a simple yet very efficient
approach to fitting linear classifiers and regressors under
convex loss functions such as (linear) <a class="reference external" href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic
Regression</a>.
Even though SGD has been around in the machine learning community for
a long time, it has received a considerable amount of attention just
recently in the context of large-scale learning.</p>
<p>SGD has been successfully applied to large-scale and sparse machine
learning problems often encountered in text classification and natural
language processing.  Given that the data is sparse, the classifiers
in this module easily scale to problems with more than 10^5 training
examples and more than 10^5 features.</p>
<p>Strictly speaking, SGD is merely an optimization technique and does not
correspond to a specific family of machine learning models. It is only a
<em>way</em> to train a model. Often, an instance of <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> or
<a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> will have an equivalent estimator in
the scikit-learn API, potentially using a different optimization technique.
For example, using <code class="docutils literal notranslate"><span class="pre">SGDClassifier(loss='log_loss')</span></code> results in logistic regression,
i.e. a model equivalent to <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a>
which is fitted via SGD instead of being fitted by one of the other solvers
in <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a>. Similarly,
<code class="docutils literal notranslate"><span class="pre">SGDRegressor(loss='squared_error',</span> <span class="pre">penalty='l2')</span></code> and
<a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> solve the same optimization problem, via
different means.</p>
<p>The advantages of Stochastic Gradient Descent are:</p>
<ul class="simple">
<li><p>Efficiency.</p></li>
<li><p>Ease of implementation (lots of opportunities for code tuning).</p></li>
</ul>
<p>The disadvantages of Stochastic Gradient Descent include:</p>
<ul class="simple">
<li><p>SGD requires a number of hyperparameters such as the regularization
parameter and the number of iterations.</p></li>
<li><p>SGD is sensitive to feature scaling.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Make sure you permute (shuffle) your training data before fitting the model
or use <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> to shuffle after each iteration (used by default).
Also, ideally, features should be standardized using e.g.
<code class="docutils literal notranslate"><span class="pre">make_pipeline(StandardScaler(),</span> <span class="pre">SGDClassifier())</span></code> (see <a class="reference internal" href="compose.html#combining-estimators"><span class="std std-ref">Pipelines</span></a>).</p>
</div>
<section id="classification">
<h2><span class="section-number">1.5.1. </span>Classification<a class="headerlink" href="#classification" title="Link to this heading">#</a></h2>
<p>The class <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> implements a plain stochastic gradient
descent learning routine which supports different loss functions and
penalties for classification. Below is the decision boundary of a
<a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> trained with the hinge loss, equivalent to a linear SVM.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html"><img alt="../_images/sphx_glr_plot_sgd_separating_hyperplane_001.png" src="../_images/sphx_glr_plot_sgd_separating_hyperplane_001.png" style="width: 480.0px; height: 360.0px;" />
</a>
</figure>
<p>As other classifiers, SGD has to be fitted with two arrays: an array <code class="docutils literal notranslate"><span class="pre">X</span></code>
of shape (n_samples, n_features) holding the training samples, and an
array y of shape (n_samples,) holding the target values (class labels)
for the training samples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SGDClassifier(max_iter=5)</span>
</pre></div>
</div>
<p>After being fitted, the model can then be used to predict new values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
<p>SGD fits a linear model to the training data. The <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute holds
the model parameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([[9.9..., 9.9...]])</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> attribute holds the intercept (aka offset or bias):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>
<span class="go">array([-9.9...])</span>
</pre></div>
</div>
<p>Whether or not the model should use an intercept, i.e. a biased
hyperplane, is controlled by the parameter <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code>.</p>
<p>The signed distance to the hyperplane (computed as the dot product between
the coefficients and the input sample, plus the intercept) is given by
<a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function" title="sklearn.linear_model.SGDClassifier.decision_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">SGDClassifier.decision_function</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([29.6...])</span>
</pre></div>
</div>
<p>The concrete loss function can be set via the <code class="docutils literal notranslate"><span class="pre">loss</span></code>
parameter. <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> supports the following loss functions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loss=&quot;hinge&quot;</span></code>: (soft-margin) linear Support Vector Machine,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss=&quot;modified_huber&quot;</span></code>: smoothed hinge loss,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss=&quot;log_loss&quot;</span></code>: logistic regression,</p></li>
<li><p>and all regression losses below. In this case the target is encoded as -1
or 1, and the problem is treated as a regression problem. The predicted
class then correspond to the sign of the predicted target.</p></li>
</ul>
<p>Please refer to the <a class="reference internal" href="#sgd-mathematical-formulation"><span class="std std-ref">mathematical section below</span></a> for formulas.
The first two loss functions are lazy, they only update the model
parameters if an example violates the margin constraint, which makes
training very efficient and may result in sparser models (i.e. with more zero
coefficients), even when L2 penalty is used.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">loss=&quot;log_loss&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">loss=&quot;modified_huber&quot;</span></code> enables the
<code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method, which gives a vector of probability estimates
<span class="math notranslate nohighlight">\(P(y|x)\)</span> per sample <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;log_loss&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span> 
<span class="go">array([[0.00..., 0.99...]])</span>
</pre></div>
</div>
<p>The concrete penalty can be set via the <code class="docutils literal notranslate"><span class="pre">penalty</span></code> parameter.
SGD supports the following penalties:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">penalty=&quot;l2&quot;</span></code>: L2 norm penalty on <code class="docutils literal notranslate"><span class="pre">coef_</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">penalty=&quot;l1&quot;</span></code>: L1 norm penalty on <code class="docutils literal notranslate"><span class="pre">coef_</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">penalty=&quot;elasticnet&quot;</span></code>: Convex combination of L2 and L1;
<code class="docutils literal notranslate"><span class="pre">(1</span> <span class="pre">-</span> <span class="pre">l1_ratio)</span> <span class="pre">*</span> <span class="pre">L2</span> <span class="pre">+</span> <span class="pre">l1_ratio</span> <span class="pre">*</span> <span class="pre">L1</span></code>.</p></li>
</ul>
<p>The default setting is <code class="docutils literal notranslate"><span class="pre">penalty=&quot;l2&quot;</span></code>. The L1 penalty leads to sparse
solutions, driving most coefficients to zero. The Elastic Net <a class="footnote-reference brackets" href="#id15" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> solves
some deficiencies of the L1 penalty in the presence of highly correlated
attributes. The parameter <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> controls the convex combination
of L1 and L2 penalty.</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> supports multi-class classification by combining
multiple binary classifiers in a “one versus all” (OVA) scheme. For each
of the <span class="math notranslate nohighlight">\(K\)</span> classes, a binary classifier is learned that discriminates
between that and all other <span class="math notranslate nohighlight">\(K-1\)</span> classes. At testing time, we compute the
confidence score (i.e. the signed distances to the hyperplane) for each
classifier and choose the class with the highest confidence. The Figure
below illustrates the OVA approach on the iris dataset.  The dashed
lines represent the three OVA classifiers; the background colors show
the decision surface induced by the three classifiers.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_iris.html"><img alt="../_images/sphx_glr_plot_sgd_iris_001.png" src="../_images/sphx_glr_plot_sgd_iris_001.png" style="width: 480.0px; height: 360.0px;" />
</a>
</figure>
<p>In the case of multi-class classification <code class="docutils literal notranslate"><span class="pre">coef_</span></code> is a two-dimensional
array of shape (n_classes, n_features) and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> is a
one-dimensional array of shape (n_classes,). The i-th row of <code class="docutils literal notranslate"><span class="pre">coef_</span></code> holds
the weight vector of the OVA classifier for the i-th class; classes are
indexed in ascending order (see attribute <code class="docutils literal notranslate"><span class="pre">classes_</span></code>).
Note that, in principle, since they allow to create a probability model,
<code class="docutils literal notranslate"><span class="pre">loss=&quot;log_loss&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">loss=&quot;modified_huber&quot;</span></code> are more suitable for
one-vs-all classification.</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> supports both weighted classes and weighted
instances via the fit parameters <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> and <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>. See
the examples below and the docstring of <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit" title="sklearn.linear_model.SGDClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">SGDClassifier.fit</span></code></a> for
further information.</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> supports averaged SGD (ASGD) <a class="footnote-reference brackets" href="#id14" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. Averaging can be
enabled by setting <code class="docutils literal notranslate"><span class="pre">average=True</span></code>. ASGD performs the same updates as the
regular SGD (see <a class="reference internal" href="#sgd-mathematical-formulation"><span class="std std-ref">Mathematical formulation</span></a>), but instead of using
the last value of the coefficients as the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute (i.e. the values
of the last update), <code class="docutils literal notranslate"><span class="pre">coef_</span></code> is set instead to the <strong>average</strong> value of the
coefficients across all updates. The same is done for the <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>
attribute. When using ASGD the learning rate can be larger and even constant,
leading on some datasets to a speed up in training time.</p>
<p>For classification with a logistic loss, another variant of SGD with an
averaging strategy is available with Stochastic Average Gradient (SAG)
algorithm, available as a solver in <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a>.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py"><span class="std std-ref">SGD: Maximum margin separating hyperplane</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_iris.html#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py"><span class="std std-ref">Plot multi-class SGD on the iris dataset</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py"><span class="std std-ref">SGD: Weighted samples</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_comparison.html#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py"><span class="std std-ref">Comparing various online solvers</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><span class="std std-ref">SVM: Separating hyperplane for unbalanced classes</span></a>
(See the Note in the example)</p></li>
</ul>
</section>
<section id="regression">
<h2><span class="section-number">1.5.2. </span>Regression<a class="headerlink" href="#regression" title="Link to this heading">#</a></h2>
<p>The class <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> implements a plain stochastic gradient
descent learning routine which supports different loss functions and
penalties to fit linear regression models. <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> is
well suited for regression problems with a large number of training
samples (&gt; 10.000), for other problems we recommend <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a>,
<a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a>, or <a class="reference internal" href="generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>.</p>
<p>The concrete loss function can be set via the <code class="docutils literal notranslate"><span class="pre">loss</span></code>
parameter. <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> supports the following loss functions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loss=&quot;squared_error&quot;</span></code>: Ordinary least squares,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss=&quot;huber&quot;</span></code>: Huber loss for robust regression,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss=&quot;epsilon_insensitive&quot;</span></code>: linear Support Vector Regression.</p></li>
</ul>
<p>Please refer to the <a class="reference internal" href="#sgd-mathematical-formulation"><span class="std std-ref">mathematical section below</span></a> for formulas.
The Huber and epsilon-insensitive loss functions can be used for
robust regression. The width of the insensitive region has to be
specified via the parameter <code class="docutils literal notranslate"><span class="pre">epsilon</span></code>. This parameter depends on the
scale of the target variables.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">penalty</span></code> parameter determines the regularization to be used (see
description above in the classification section).</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> also supports averaged SGD <a class="footnote-reference brackets" href="#id14" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> (here again, see
description above in the classification section).</p>
<p>For regression with a squared loss and a l2 penalty, another variant of
SGD with an averaging strategy is available with Stochastic Average
Gradient (SAG) algorithm, available as a solver in <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a>.</p>
</section>
<section id="online-one-class-svm">
<span id="sgd-online-one-class-svm"></span><h2><span class="section-number">1.5.3. </span>Online One-Class SVM<a class="headerlink" href="#online-one-class-svm" title="Link to this heading">#</a></h2>
<p>The class <a class="reference internal" href="generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM" title="sklearn.linear_model.SGDOneClassSVM"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDOneClassSVM</span></code></a> implements an online
linear version of the One-Class SVM using a stochastic gradient descent.
Combined with kernel approximation techniques,
<a class="reference internal" href="generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM" title="sklearn.linear_model.SGDOneClassSVM"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDOneClassSVM</span></code></a> can be used to approximate the
solution of a kernelized One-Class SVM, implemented in
<a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm.OneClassSVM</span></code></a>, with a linear complexity in the number of
samples. Note that the complexity of a kernelized One-Class SVM is at best
quadratic in the number of samples.
<a class="reference internal" href="generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM" title="sklearn.linear_model.SGDOneClassSVM"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDOneClassSVM</span></code></a> is thus well suited for datasets
with a large number of training samples (&gt; 10,000) for which the SGD
variant can be several orders of magnitude faster.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" id="mathematical-details">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Mathematical details<a class="headerlink" href="#mathematical-details" title="Link to this dropdown">#</a></span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Its implementation is based on the implementation of the stochastic
gradient descent. Indeed, the original optimization problem of the One-Class
SVM is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\min_{w, \rho, \xi} &amp; \quad \frac{1}{2}\Vert w \Vert^2 - \rho + \frac{1}{\nu n} \sum_{i=1}^n \xi_i \\
\text{s.t.} &amp; \quad \langle w, x_i \rangle \geq \rho - \xi_i \quad 1 \leq i \leq n \\
&amp; \quad \xi_i \geq 0 \quad 1 \leq i \leq n
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\nu \in (0, 1]\)</span> is the user-specified parameter controlling the
proportion of outliers and the proportion of support vectors. Getting rid of
the slack variables <span class="math notranslate nohighlight">\(\xi_i\)</span> this problem is equivalent to</p>
<div class="math notranslate nohighlight">
\[\min_{w, \rho} \frac{1}{2}\Vert w \Vert^2 - \rho + \frac{1}{\nu n} \sum_{i=1}^n \max(0, \rho - \langle w, x_i \rangle) \, .\]</div>
<p class="sd-card-text">Multiplying by the constant <span class="math notranslate nohighlight">\(\nu\)</span> and introducing the intercept
<span class="math notranslate nohighlight">\(b = 1 - \rho\)</span> we obtain the following equivalent optimization problem</p>
<div class="math notranslate nohighlight">
\[\min_{w, b} \frac{\nu}{2}\Vert w \Vert^2 + b\nu + \frac{1}{n} \sum_{i=1}^n \max(0, 1 - (\langle w, x_i \rangle + b)) \, .\]</div>
<p class="sd-card-text">This is similar to the optimization problems studied in section
<a class="reference internal" href="#sgd-mathematical-formulation"><span class="std std-ref">Mathematical formulation</span></a> with <span class="math notranslate nohighlight">\(y_i = 1, 1 \leq i \leq n\)</span> and
<span class="math notranslate nohighlight">\(\alpha = \nu/2\)</span>, <span class="math notranslate nohighlight">\(L\)</span> being the hinge loss function and <span class="math notranslate nohighlight">\(R\)</span>
being the L2 norm. We just need to add the term <span class="math notranslate nohighlight">\(b\nu\)</span> in the
optimization loop.</p>
</div>
</details><p>As <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> and <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a>, <a class="reference internal" href="generated/sklearn.linear_model.SGDOneClassSVM.html#sklearn.linear_model.SGDOneClassSVM" title="sklearn.linear_model.SGDOneClassSVM"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDOneClassSVM</span></code></a>
supports averaged SGD. Averaging can be enabled by setting <code class="docutils literal notranslate"><span class="pre">average=True</span></code>.</p>
</section>
<section id="stochastic-gradient-descent-for-sparse-data">
<h2><span class="section-number">1.5.4. </span>Stochastic Gradient Descent for sparse data<a class="headerlink" href="#stochastic-gradient-descent-for-sparse-data" title="Link to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The sparse implementation produces slightly different results
from the dense implementation, due to a shrunk learning rate for the
intercept. See <a class="reference internal" href="#implementation-details"><span class="std std-ref">Implementation details</span></a>.</p>
</div>
<p>There is built-in support for sparse data given in any matrix in a format
supported by <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/sparse.html">scipy.sparse</a>. For maximum
efficiency, however, use the CSR
matrix format as defined in <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a>.</p>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></p></li>
</ul>
</section>
<section id="complexity">
<h2><span class="section-number">1.5.5. </span>Complexity<a class="headerlink" href="#complexity" title="Link to this heading">#</a></h2>
<p>The major advantage of SGD is its efficiency, which is basically
linear in the number of training examples. If X is a matrix of size (n, p)
training has a cost of <span class="math notranslate nohighlight">\(O(k n \bar p)\)</span>, where k is the number
of iterations (epochs) and <span class="math notranslate nohighlight">\(\bar p\)</span> is the average number of
non-zero attributes per sample.</p>
<p>Recent theoretical results, however, show that the runtime to get some
desired optimization accuracy does not increase as the training set size increases.</p>
</section>
<section id="stopping-criterion">
<h2><span class="section-number">1.5.6. </span>Stopping criterion<a class="headerlink" href="#stopping-criterion" title="Link to this heading">#</a></h2>
<p>The classes <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> and <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> provide two
criteria to stop the algorithm when a given level of convergence is reached:</p>
<ul class="simple">
<li><p>With <code class="docutils literal notranslate"><span class="pre">early_stopping=True</span></code>, the input data is split into a training set
and a validation set. The model is then fitted on the training set, and the
stopping criterion is based on the prediction score (using the <code class="docutils literal notranslate"><span class="pre">score</span></code>
method) computed on the validation set. The size of the validation set
can be changed with the parameter <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code>.</p></li>
<li><p>With <code class="docutils literal notranslate"><span class="pre">early_stopping=False</span></code>, the model is fitted on the entire input data
and the stopping criterion is based on the objective function computed on
the training data.</p></li>
</ul>
<p>In both cases, the criterion is evaluated once by epoch, and the algorithm stops
when the criterion does not improve <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> times in a row. The
improvement is evaluated with absolute tolerance <code class="docutils literal notranslate"><span class="pre">tol</span></code>, and the algorithm
stops in any case after a maximum number of iteration <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>.</p>
</section>
<section id="tips-on-practical-use">
<h2><span class="section-number">1.5.7. </span>Tips on Practical Use<a class="headerlink" href="#tips-on-practical-use" title="Link to this heading">#</a></h2>
<ul>
<li><p>Stochastic Gradient Descent is sensitive to feature scaling, so it
is highly recommended to scale your data. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize
it to have mean 0 and variance 1. Note that the <em>same</em> scaling must be
applied to the test vector to obtain meaningful results. This can be easily
done using <a class="reference internal" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># Don&#39;t cheat - fit only on training data</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  <span class="c1"># apply same transformation to test data</span>

<span class="c1"># Or better yet: use a pipeline!</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="n">est</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SGDClassifier</span><span class="p">())</span>
<span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>If your attributes have an intrinsic scale (e.g. word frequencies or
indicator features) scaling is not needed.</p>
</li>
<li><p>Finding a reasonable regularization term <span class="math notranslate nohighlight">\(\alpha\)</span> is
best done using automatic hyper-parameter search, e.g.
<a class="reference internal" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">GridSearchCV</span></code></a> or
<a class="reference internal" href="generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV" title="sklearn.model_selection.RandomizedSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomizedSearchCV</span></code></a>, usually in the
range <code class="docutils literal notranslate"><span class="pre">10.0**-np.arange(1,7)</span></code>.</p></li>
<li><p>Empirically, we found that SGD converges after observing
approximately 10^6 training samples. Thus, a reasonable first guess
for the number of iterations is <code class="docutils literal notranslate"><span class="pre">max_iter</span> <span class="pre">=</span> <span class="pre">np.ceil(10**6</span> <span class="pre">/</span> <span class="pre">n)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">n</span></code> is the size of the training set.</p></li>
<li><p>If you apply SGD to features extracted using PCA we found that
it is often wise to scale the feature values by some constant <code class="docutils literal notranslate"><span class="pre">c</span></code>
such that the average L2 norm of the training data equals one.</p></li>
<li><p>We found that Averaged SGD works best with a larger number of features
and a higher eta0.</p></li>
</ul>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a>
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.</p></li>
</ul>
</section>
<section id="mathematical-formulation">
<span id="sgd-mathematical-formulation"></span><h2><span class="section-number">1.5.8. </span>Mathematical formulation<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h2>
<p>We describe here the mathematical details of the SGD procedure. A good
overview with convergence rates can be found in <a class="footnote-reference brackets" href="#id16" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>.</p>
<p>Given a set of training examples <span class="math notranslate nohighlight">\((x_1, y_1), \ldots, (x_n, y_n)\)</span> where
<span class="math notranslate nohighlight">\(x_i \in \mathbf{R}^m\)</span> and <span class="math notranslate nohighlight">\(y_i \in \mathcal{R}\)</span> (<span class="math notranslate nohighlight">\(y_i \in
{-1, 1}\)</span> for classification), our goal is to learn a linear scoring function
<span class="math notranslate nohighlight">\(f(x) = w^T x + b\)</span> with model parameters <span class="math notranslate nohighlight">\(w \in \mathbf{R}^m\)</span> and
intercept <span class="math notranslate nohighlight">\(b \in \mathbf{R}\)</span>. In order to make predictions for binary
classification, we simply look at the sign of <span class="math notranslate nohighlight">\(f(x)\)</span>. To find the model
parameters, we minimize the regularized training error given by</p>
<div class="math notranslate nohighlight">
\[E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is a loss function that measures model (mis)fit and
<span class="math notranslate nohighlight">\(R\)</span> is a regularization term (aka penalty) that penalizes model
complexity; <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> is a non-negative hyperparameter that controls
the regularization strength.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" id="loss-functions-details">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Loss functions details<a class="headerlink" href="#loss-functions-details" title="Link to this dropdown">#</a></span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Different choices for <span class="math notranslate nohighlight">\(L\)</span> entail different classifiers or regressors:</p>
<ul class="simple">
<li><p class="sd-card-text">Hinge (soft-margin): equivalent to Support Vector Classification.
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\)</span>.</p></li>
<li><p class="sd-card-text">Perceptron:
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \max(0, - y_i f(x_i))\)</span>.</p></li>
<li><p class="sd-card-text">Modified Huber:
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\)</span> if <span class="math notranslate nohighlight">\(y_i f(x_i) &gt;
-1\)</span>, and <span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = -4 y_i f(x_i)\)</span> otherwise.</p></li>
<li><p class="sd-card-text">Log Loss: equivalent to Logistic Regression.
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\)</span>.</p></li>
<li><p class="sd-card-text">Squared Error: Linear regression (Ridge or Lasso depending on
<span class="math notranslate nohighlight">\(R\)</span>).
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\)</span>.</p></li>
<li><p class="sd-card-text">Huber: less sensitive to outliers than least-squares. It is equivalent to
least squares when <span class="math notranslate nohighlight">\(|y_i - f(x_i)| \leq \varepsilon\)</span>, and
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \varepsilon |y_i - f(x_i)| - \frac{1}{2}
\varepsilon^2\)</span> otherwise.</p></li>
<li><p class="sd-card-text">Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression.
<span class="math notranslate nohighlight">\(L(y_i, f(x_i)) = \max(0, |y_i - f(x_i)| - \varepsilon)\)</span>.</p></li>
</ul>
</div>
</details><p>All of the above loss functions can be regarded as an upper bound on the
misclassification error (Zero-one loss) as shown in the Figure below.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_loss_functions.html"><img alt="../_images/sphx_glr_plot_sgd_loss_functions_001.png" src="../_images/sphx_glr_plot_sgd_loss_functions_001.png" style="width: 480.0px; height: 360.0px;" />
</a>
</figure>
<p>Popular choices for the regularization term <span class="math notranslate nohighlight">\(R\)</span> (the <code class="docutils literal notranslate"><span class="pre">penalty</span></code>
parameter) include:</p>
<ul class="simple">
<li><p>L2 norm: <span class="math notranslate nohighlight">\(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\)</span>,</p></li>
<li><p>L1 norm: <span class="math notranslate nohighlight">\(R(w) := \sum_{j=1}^{m} |w_j|\)</span>, which leads to sparse
solutions.</p></li>
<li><p>Elastic Net: <span class="math notranslate nohighlight">\(R(w) := \frac{\rho}{2} \sum_{j=1}^{n} w_j^2 +
(1-\rho) \sum_{j=1}^{m} |w_j|\)</span>, a convex combination of L2 and L1, where
<span class="math notranslate nohighlight">\(\rho\)</span> is given by <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">l1_ratio</span></code>.</p></li>
</ul>
<p>The Figure below shows the contours of the different regularization terms
in a 2-dimensional parameter space (<span class="math notranslate nohighlight">\(m=2\)</span>) when <span class="math notranslate nohighlight">\(R(w) = 1\)</span>.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_penalties.html"><img alt="../_images/sphx_glr_plot_sgd_penalties_001.png" src="../_images/sphx_glr_plot_sgd_penalties_001.png" style="width: 750.0px; height: 750.0px;" />
</a>
</figure>
<section id="id5">
<h3><span class="section-number">1.5.8.1. </span>SGD<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>Stochastic gradient descent is an optimization method for unconstrained
optimization problems. In contrast to (batch) gradient descent, SGD
approximates the true gradient of <span class="math notranslate nohighlight">\(E(w,b)\)</span> by considering a
single training example at a time.</p>
<p>The class <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> implements a first-order SGD learning
routine.  The algorithm iterates over the training examples and for each
example updates the model parameters according to the update rule given by</p>
<div class="math notranslate nohighlight">
\[w \leftarrow w - \eta \left[\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w}\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate which controls the step-size in
the parameter space.  The intercept <span class="math notranslate nohighlight">\(b\)</span> is updated similarly but
without regularization (and with additional decay for sparse matrices, as
detailed in <a class="reference internal" href="#implementation-details"><span class="std std-ref">Implementation details</span></a>).</p>
<p>The learning rate <span class="math notranslate nohighlight">\(\eta\)</span> can be either constant or gradually decaying. For
classification, the default learning rate schedule (<code class="docutils literal notranslate"><span class="pre">learning_rate='optimal'</span></code>)
is given by</p>
<div class="math notranslate nohighlight">
\[\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> is the time step (there are a total of <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">n_iter</span></code>
time steps), <span class="math notranslate nohighlight">\(t_0\)</span> is determined based on a heuristic proposed by Léon Bottou
such that the expected initial updates are comparable with the expected
size of the weights (this assuming that the norm of the training samples is
approx. 1). The exact definition can be found in <code class="docutils literal notranslate"><span class="pre">_init_t</span></code> in <code class="docutils literal notranslate"><span class="pre">BaseSGD</span></code>.</p>
<p>For regression the default learning rate schedule is inverse scaling
(<code class="docutils literal notranslate"><span class="pre">learning_rate='invscaling'</span></code>), given by</p>
<div class="math notranslate nohighlight">
\[\eta^{(t)} = \frac{eta_0}{t^{power\_t}}\]</div>
<p>where <span class="math notranslate nohighlight">\(eta_0\)</span> and <span class="math notranslate nohighlight">\(power\_t\)</span> are hyperparameters chosen by the
user via <code class="docutils literal notranslate"><span class="pre">eta0</span></code> and <code class="docutils literal notranslate"><span class="pre">power_t</span></code>, resp.</p>
<p>For a constant learning rate use <code class="docutils literal notranslate"><span class="pre">learning_rate='constant'</span></code> and use <code class="docutils literal notranslate"><span class="pre">eta0</span></code>
to specify the learning rate.</p>
<p>For an adaptively decreasing learning rate, use <code class="docutils literal notranslate"><span class="pre">learning_rate='adaptive'</span></code>
and use <code class="docutils literal notranslate"><span class="pre">eta0</span></code> to specify the starting learning rate. When the stopping
criterion is reached, the learning rate is divided by 5, and the algorithm
does not stop. The algorithm stops when the learning rate goes below 1e-6.</p>
<p>The model parameters can be accessed through the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and
<code class="docutils literal notranslate"><span class="pre">intercept_</span></code> attributes: <code class="docutils literal notranslate"><span class="pre">coef_</span></code> holds the weights <span class="math notranslate nohighlight">\(w\)</span> and
<code class="docutils literal notranslate"><span class="pre">intercept_</span></code> holds <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>When using Averaged SGD (with the <code class="docutils literal notranslate"><span class="pre">average</span></code> parameter), <code class="docutils literal notranslate"><span class="pre">coef_</span></code> is set to the
average weight across all updates:
<code class="docutils literal notranslate"><span class="pre">coef_</span></code> <span class="math notranslate nohighlight">\(= \frac{1}{T} \sum_{t=0}^{T-1} w^{(t)}\)</span>,
where <span class="math notranslate nohighlight">\(T\)</span> is the total number of updates, found in the <code class="docutils literal notranslate"><span class="pre">t_</span></code> attribute.</p>
</section>
</section>
<section id="implementation-details">
<span id="id6"></span><h2><span class="section-number">1.5.9. </span>Implementation details<a class="headerlink" href="#implementation-details" title="Link to this heading">#</a></h2>
<p>The implementation of SGD is influenced by the <code class="docutils literal notranslate"><span class="pre">Stochastic</span> <span class="pre">Gradient</span> <span class="pre">SVM</span></code> of
<a class="footnote-reference brackets" href="#id10" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>.
Similar to SvmSGD,
the weight vector is represented as the product of a scalar and a vector
which allows an efficient weight update in the case of L2 regularization.
In the case of sparse input <code class="docutils literal notranslate"><span class="pre">X</span></code>, the intercept is updated with a
smaller learning rate (multiplied by 0.01) to account for the fact that
it is updated more frequently. Training examples are picked up sequentially
and the learning rate is lowered after each observed example. We adopted the
learning rate schedule from <a class="footnote-reference brackets" href="#id12" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>.
For multi-class classification, a “one versus all” approach is used.
We use the truncated gradient algorithm proposed in <a class="footnote-reference brackets" href="#id13" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>
for L1 regularization (and the Elastic Net).
The code is written in Cython.</p>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</p>
</aside>
<aside class="footnote brackets" id="id12" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">8</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1145/1273496.1273598">“Pegasos: Primal estimated sub-gradient solver for svm”</a>
S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.</p>
</aside>
<aside class="footnote brackets" id="id13" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">9</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.aclweb.org/anthology/P/P09/P09-1054.pdf">“Stochastic gradient descent training for l1-regularized
log-linear models with cumulative penalty”</a>
Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL’09.</p>
</aside>
<aside class="footnote brackets" id="id14" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p><a class="reference external" href="https://arxiv.org/abs/1107.2490v2">“Towards Optimal One Pass Large Scale Learning with
Averaged Stochastic Gradient Descent”</a>. Xu, Wei (2011)</p>
</aside>
<aside class="footnote brackets" id="id15" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">11</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1111/j.1467-9868.2005.00503.x">“Regularization and variable selection via the elastic net”</a>
H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,
67 (2), 301-320.</p>
</aside>
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">12</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1145/1015330.1015332">“Solving large scale linear prediction problems using stochastic
gradient descent algorithms”</a>
T. Zhang - In Proceedings of ICML ‘04.</p>
</aside>
</aside>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="prev-next-area">
    <a class="left-prev"
       href="svm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1.4. </span>Support Vector Machines</p>
      </div>
    </a>
    <a class="right-next"
       href="neighbors.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.6. </span>Nearest Neighbors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>
                </footer>
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification">1.5.1. Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression">1.5.2. Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#online-one-class-svm">1.5.3. Online One-Class SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-for-sparse-data">1.5.4. Stochastic Gradient Descent for sparse data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complexity">1.5.5. Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criterion">1.5.6. Stopping criterion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-on-practical-use">1.5.7. Tips on Practical Use</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">1.5.8. Mathematical formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">1.5.8.1. SGD</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-details">1.5.9. Implementation details</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/modules/sgd.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>