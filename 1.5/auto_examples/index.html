
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Examples" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://scikit-learn/stable/auto_examples/index.html" />
<meta property="og:site_name" content="scikit-learn" />
<meta property="og:description" content="This is the gallery of examples that showcase how scikit-learn can be used. Some examples demonstrate the use of the API in general and some demonstrate specific applications in tutorial form. Also..." />
<meta property="og:image" content="https://scikit-learn/stable/_images/sphx_glr_plot_release_highlights_1_5_0_thumb.png" />
<meta property="og:image:alt" content="" />
<meta name="description" content="This is the gallery of examples that showcase how scikit-learn can be used. Some examples demonstrate the use of the API in general and some demonstrate specific applications in tutorial form. Also..." />

    <title>Examples &#8212; scikit-learn 1.5.2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Vibur" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyterlite_sphinx.css?v=ca70e7f1" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/colors.css?v=cc94ab7d" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/custom.css?v=e4cb1417" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=73275c37"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=97f0b27d"></script>
    <script src="../_static/jupyterlite_sphinx.js?v=d6bdf5f8"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script data-domain="scikit-learn.org" defer="defer" src="https://views.scientific-python.org/js/script.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'auto_examples/index';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://scikit-learn.org/dev/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.5.2';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../_static/scripts/dropdown.js?v=e2048168"></script>
    <script src="../_static/scripts/version-switcher.js?v=a6dd8357"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Release Highlights" href="release_highlights/index.html" />
    <link rel="prev" title="register_parallel_backend" href="../modules/generated/sklearn.utils.register_parallel_backend.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/scikit-learn-logo-small.png" class="logo__image only-light" alt="scikit-learn homepage"/>
    <script>document.write(`<img src="../_static/scikit-learn-logo-small.png" class="logo__image only-dark" alt="scikit-learn homepage"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install.html">
    Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/index.html">
    API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://blog.scikit-learn.org/">
    Community
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../getting_started.html">
    Getting Started
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../whats_new.html">
    Release History
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../glossary.html">
    Glossary
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://scikit-learn.org/dev/developers/index.html">
    Development
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../support.html">
    Support
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../related_projects.html">
    Related Projects
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../roadmap.html">
    Roadmap
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../governance.html">
    Governance
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../about.html">
    About us
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-learn/scikit-learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install.html">
    Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/index.html">
    API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://blog.scikit-learn.org/">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../whats_new.html">
    Release History
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../glossary.html">
    Glossary
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://scikit-learn.org/dev/developers/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../support.html">
    Support
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../related_projects.html">
    Related Projects
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../roadmap.html">
    Roadmap
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../governance.html">
    Governance
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about.html">
    About us
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-learn/scikit-learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="release_highlights/index.html">Release Highlights</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_1_5_0.html">Release Highlights for scikit-learn 1.5</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_1_4_0.html">Release Highlights for scikit-learn 1.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_1_3_0.html">Release Highlights for scikit-learn 1.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_1_2_0.html">Release Highlights for scikit-learn 1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_1_1_0.html">Release Highlights for scikit-learn 1.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_1_0_0.html">Release Highlights for scikit-learn 1.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_0_24_0.html">Release Highlights for scikit-learn 0.24</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_0_23_0.html">Release Highlights for scikit-learn 0.23</a></li>
<li class="toctree-l2"><a class="reference internal" href="release_highlights/plot_release_highlights_0_22_0.html">Release Highlights for scikit-learn 0.22</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="bicluster/index.html">Biclustering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="bicluster/plot_spectral_biclustering.html">A demo of the Spectral Biclustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="bicluster/plot_spectral_coclustering.html">A demo of the Spectral Co-Clustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="bicluster/plot_bicluster_newsgroups.html">Biclustering documents with the Spectral Co-clustering algorithm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="calibration/index.html">Calibration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="calibration/plot_compare_calibration.html">Comparison of Calibration of Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="calibration/plot_calibration_curve.html">Probability Calibration curves</a></li>
<li class="toctree-l2"><a class="reference internal" href="calibration/plot_calibration_multiclass.html">Probability Calibration for 3-class classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="calibration/plot_calibration.html">Probability calibration of classifiers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="classification/index.html">Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="classification/plot_classifier_comparison.html">Classifier comparison</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification/plot_lda_qda.html">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification/plot_lda.html">Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification/plot_classification_probability.html">Plot classification probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification/plot_digits_classification.html">Recognizing hand-written digits</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cluster/index.html">Clustering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_kmeans_digits.html">A demo of K-Means clustering on the handwritten digits data</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_coin_ward_segmentation.html">A demo of structured Ward hierarchical clustering on an image of coins</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_mean_shift.html">A demo of the mean-shift clustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_adjusted_for_chance_measures.html">Adjustment for chance in clustering performance evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_agglomerative_clustering.html">Agglomerative clustering with and without structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_agglomerative_clustering_metrics.html">Agglomerative clustering with different metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_kmeans_plusplus.html">An example of K-Means++ initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_bisect_kmeans.html">Bisecting K-Means and Regular K-Means Performance Comparison</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_color_quantization.html">Color Quantization using K-Means</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_birch_vs_minibatchkmeans.html">Compare BIRCH and MiniBatchKMeans</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_cluster_comparison.html">Comparing different clustering algorithms on toy datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_linkage_comparison.html">Comparing different hierarchical linkage methods on toy datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_mini_batch_kmeans.html">Comparison of the K-Means and MiniBatchKMeans clustering algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_dbscan.html">Demo of DBSCAN clustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_hdbscan.html">Demo of HDBSCAN clustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_optics.html">Demo of OPTICS clustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_affinity_propagation.html">Demo of affinity propagation clustering algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_kmeans_assumptions.html">Demonstration of k-means assumptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_kmeans_stability_low_dim_dense.html">Empirical evaluation of the impact of k-means initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_digits_agglomeration.html">Feature agglomeration</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_feature_agglomeration_vs_univariate_selection.html">Feature agglomeration vs. univariate selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_ward_structured_vs_unstructured.html">Hierarchical clustering: structured vs unstructured ward</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_inductive_clustering.html">Inductive Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_cluster_iris.html">K-means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_dict_face_patches.html">Online learning of a dictionary of parts of faces</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_agglomerative_dendrogram.html">Plot Hierarchical Clustering Dendrogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_coin_segmentation.html">Segmenting the picture of greek coins in regions</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_kmeans_silhouette_analysis.html">Selecting the number of clusters with silhouette analysis on KMeans clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_segmentation_toy.html">Spectral clustering for image segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_digits_linkage.html">Various Agglomerative Clustering on a 2D embedding of digits</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster/plot_face_compress.html">Vector Quantization Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="covariance/index.html">Covariance estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="covariance/plot_lw_vs_oas.html">Ledoit-Wolf vs OAS estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="covariance/plot_mahalanobis_distances.html">Robust covariance estimation and Mahalanobis distances relevance</a></li>
<li class="toctree-l2"><a class="reference internal" href="covariance/plot_robust_vs_empirical_covariance.html">Robust vs Empirical covariance estimate</a></li>
<li class="toctree-l2"><a class="reference internal" href="covariance/plot_covariance_estimation.html">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="covariance/plot_sparse_cov.html">Sparse inverse covariance estimation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cross_decomposition/index.html">Cross decomposition</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="cross_decomposition/plot_compare_cross_decomposition.html">Compare cross decomposition methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="cross_decomposition/plot_pcr_vs_pls.html">Principal Component Regression vs Partial Least Squares Regression</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="datasets/index.html">Dataset examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="datasets/plot_random_dataset.html">Plot randomly generated classification dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets/plot_random_multilabel_dataset.html">Plot randomly generated multilabel dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets/plot_digits_last_image.html">The Digit Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets/plot_iris_dataset.html">The Iris Dataset</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tree/index.html">Decision Trees</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="tree/plot_tree_regression.html">Decision Tree Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree/plot_tree_regression_multioutput.html">Multi-output Decision Tree Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree/plot_iris_dtc.html">Plot the decision surface of decision trees trained on the iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree/plot_cost_complexity_pruning.html">Post pruning decision trees with cost complexity pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree/plot_unveil_tree_structure.html">Understanding the decision tree structure</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="decomposition/index.html">Decomposition</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_ica_blind_source_separation.html">Blind source separation using FastICA</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_pca_vs_lda.html">Comparison of LDA and PCA 2D projection of Iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_faces_decomposition.html">Faces dataset decompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_varimax_fa.html">Factor Analysis (with rotation) to visualize patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_ica_vs_pca.html">FastICA on 2D point clouds</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_image_denoising.html">Image denoising using dictionary learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_incremental_pca.html">Incremental PCA</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_kernel_pca.html">Kernel PCA</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_pca_vs_fa_model_selection.html">Model selection with Probabilistic PCA and Factor Analysis (FA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_pca_iris.html">PCA example with Iris Data-set</a></li>
<li class="toctree-l2"><a class="reference internal" href="decomposition/plot_sparse_coding.html">Sparse coding with a precomputed dictionary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="developing_estimators/index.html">Developing Estimators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="developing_estimators/sklearn_is_fitted.html"><code class="docutils literal notranslate"><span class="pre">__sklearn_is_fitted__</span></code> as Developer API</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ensemble/index.html">Ensemble methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_gradient_boosting_categorical.html">Categorical Feature Support in Gradient Boosting</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_stack_predictors.html">Combine predictors using stacking</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_forest_hist_grad_boosting_comparison.html">Comparing Random Forests and Histogram Gradient Boosting models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_random_forest_regression_multioutput.html">Comparing random forests and the multi-output meta estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_adaboost_regression.html">Decision Tree Regression with AdaBoost</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_gradient_boosting_early_stopping.html">Early stopping in Gradient Boosting</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_forest_importances.html">Feature importances with a forest of trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_feature_transformation.html">Feature transformations with ensembles of trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_hgbt_regression.html">Features in Histogram Gradient Boosting Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_gradient_boosting_oob.html">Gradient Boosting Out-of-Bag estimates</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_gradient_boosting_regression.html">Gradient Boosting regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_gradient_boosting_regularization.html">Gradient Boosting regularization</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_random_forest_embedding.html">Hashing feature transformation using Totally Random Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_isolation_forest.html">IsolationForest example</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_monotonic_constraints.html">Monotonic Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_adaboost_multiclass.html">Multi-class AdaBoosted Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_ensemble_oob.html">OOB Errors for Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_forest_importances_faces.html">Pixel importances with a parallel forest of trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_voting_probas.html">Plot class probabilities calculated by the VotingClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_voting_regressor.html">Plot individual and voting regression predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_voting_decision_regions.html">Plot the decision boundaries of a VotingClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_forest_iris.html">Plot the decision surfaces of ensembles of trees on the iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_gradient_boosting_quantile.html">Prediction Intervals for Gradient Boosting Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_bias_variance.html">Single estimator versus bagging: bias-variance decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble/plot_adaboost_twoclass.html">Two-class AdaBoost</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="applications/index.html">Examples based on real world datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_tomography_l1_reconstruction.html">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_face_recognition.html">Faces recognition example using eigenfaces and SVMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_digits_denoising.html">Image denoising using kernel PCA</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_time_series_lagged_features.html">Lagged features for time series forecasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_model_complexity_influence.html">Model Complexity Influence</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_out_of_core_classification.html">Out-of-core classification of text documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_outlier_detection_wine.html">Outlier detection on a real data set</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_prediction_latency.html">Prediction Latency</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_species_distribution_modeling.html">Species distribution modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_cyclical_feature_engineering.html">Time-related feature engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_topics_extraction_with_nmf_lda.html">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/plot_stock_market.html">Visualizing the stock market structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications/wikipedia_principal_eigenvector.html">Wikipedia principal eigenvector</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="feature_selection/index.html">Feature Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="feature_selection/plot_f_test_vs_mi.html">Comparison of F-test and mutual information</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection/plot_select_from_model_diabetes.html">Model-based and sequential feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection/plot_feature_selection_pipeline.html">Pipeline ANOVA SVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection/plot_rfe_digits.html">Recursive feature elimination</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection/plot_rfe_with_cross_validation.html">Recursive feature elimination with cross-validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection/plot_feature_selection.html">Univariate Feature Selection</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mixture/index.html">Gaussian Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_concentration_prior.html">Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture</a></li>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_gmm_pdf.html">Density Estimation for a Gaussian mixture</a></li>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_gmm_init.html">GMM Initialization Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_gmm_covariances.html">GMM covariances</a></li>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_gmm.html">Gaussian Mixture Model Ellipsoids</a></li>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_gmm_selection.html">Gaussian Mixture Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="mixture/plot_gmm_sin.html">Gaussian Mixture Model Sine Curve</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="gaussian_process/index.html">Gaussian Process for Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpr_noisy.html">Ability of Gaussian process regression (GPR) to estimate data noise-level</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_compare_gpr_krr.html">Comparison of kernel ridge and Gaussian process regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpr_co2.html">Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpr_noisy_targets.html">Gaussian Processes regression: basic introductory example</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpc_iris.html">Gaussian process classification (GPC) on iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpr_on_structured_data.html">Gaussian processes on discrete data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpc_xor.html">Illustration of Gaussian process classification (GPC) on the XOR dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpr_prior_posterior.html">Illustration of prior and posterior Gaussian process for different kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpc_isoprobability.html">Iso-probability lines for Gaussian Processes classification (GPC)</a></li>
<li class="toctree-l2"><a class="reference internal" href="gaussian_process/plot_gpc.html">Probabilistic predictions with Gaussian process classification (GPC)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linear_model/index.html">Generalized Linear Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ard.html">Comparing Linear Bayesian Regressors</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_comparison.html">Comparing various online solvers</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_bayesian_ridge_curvefit.html">Curve Fitting with Bayesian Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_early_stopping.html">Early stopping of Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.html">Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_huber_vs_ridge.html">HuberRegressor vs Ridge on dataset with strong outliers</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_multi_task_lasso_support.html">Joint feature selection with multi-task Lasso</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_logistic_l1_l2_sparsity.html">L1 Penalty and Sparsity in Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_lasso_and_elasticnet.html">L1-based models for Sparse Signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_lasso_coordinate_descent_path.html">Lasso and Elastic Net</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_lasso_lars_ic.html">Lasso model selection via information criteria</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_lasso_model_selection.html">Lasso model selection: AIC-BIC / cross-validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_lasso_dense_vs_sparse_data.html">Lasso on dense and sparse data</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_lasso_lars.html">Lasso path using LARS</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ols.html">Linear Regression Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_iris_logistic.html">Logistic Regression 3-class Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_logistic.html">Logistic function</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sparse_logistic_regression_mnist.html">MNIST classification using multinomial logistic + L1</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sparse_logistic_regression_20newsgroups.html">Multiclass sparse logistic regression on 20newgroups</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_nnls.html">Non-negative least squares</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgdocsvm_vs_ocsvm.html">One-Class SVM versus One-Class SVM using Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ols_ridge_variance.html">Ordinary Least Squares and Ridge Regression Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_omp.html">Orthogonal Matching Pursuit</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ridge_path.html">Plot Ridge coefficients as a function of the regularization</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_iris.html">Plot multi-class SGD on the iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_logistic_multinomial.html">Plot multinomial and One-vs-Rest Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_poisson_regression_non_normal_loss.html">Poisson regression and non-normal loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_polynomial_interpolation.html">Polynomial and Spline interpolation</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_quantile_regression.html">Quantile regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_logistic_path.html">Regularization path of L1- Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ridge_coeffs.html">Ridge coefficients as a function of the L2 Regularization</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_robust_fit.html">Robust linear estimator fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ransac.html">Robust linear model estimation using RANSAC</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_separating_hyperplane.html">SGD: Maximum margin separating hyperplane</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_penalties.html">SGD: Penalties</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_weighted_samples.html">SGD: Weighted samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_sgd_loss_functions.html">SGD: convex loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_ols_3d.html">Sparsity Example: Fitting only features 1  and 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_theilsen.html">Theil-Sen Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_model/plot_tweedie_regression_insurance_claims.html">Tweedie regression on insurance claims</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="inspection/index.html">Inspection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="inspection/plot_linear_model_coefficient_interpretation.html">Common pitfalls in the interpretation of coefficients of linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="inspection/plot_causal_interpretation.html">Failure of Machine Learning to infer causal effects</a></li>
<li class="toctree-l2"><a class="reference internal" href="inspection/plot_partial_dependence.html">Partial Dependence and Individual Conditional Expectation Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="inspection/plot_permutation_importance.html">Permutation Importance vs Random Forest Feature Importance (MDI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="inspection/plot_permutation_importance_multicollinear.html">Permutation Importance with Multicollinear or Correlated Features</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="kernel_approximation/index.html">Kernel Approximation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="kernel_approximation/plot_scalable_poly_kernels.html">Scalable learning with polynomial kernel approximation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="manifold/index.html">Manifold learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="manifold/plot_compare_methods.html">Comparison of Manifold Learning methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="manifold/plot_manifold_sphere.html">Manifold Learning methods on a severed sphere</a></li>
<li class="toctree-l2"><a class="reference internal" href="manifold/plot_lle_digits.html">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</a></li>
<li class="toctree-l2"><a class="reference internal" href="manifold/plot_mds.html">Multi-dimensional scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="manifold/plot_swissroll.html">Swiss Roll And Swiss-Hole Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="manifold/plot_t_sne_perplexity.html">t-SNE: The effect of various perplexity values on the shape</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="miscellaneous/index.html">Miscellaneous</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_partial_dependence_visualization_api.html">Advanced Plotting With Partial Dependence</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_anomaly_comparison.html">Comparing anomaly detection algorithms for outlier detection on toy datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_kernel_ridge_regression.html">Comparison of kernel ridge regression and SVR</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_pipeline_display.html">Displaying Pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_estimator_representation.html">Displaying estimators and complex pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_outlier_detection_bench.html">Evaluation of outlier detection estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_kernel_approximation.html">Explicit feature map approximation for RBF kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_multioutput_face_completion.html">Face completion with a multi-output estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_set_output.html">Introducing the <code class="docutils literal notranslate"><span class="pre">set_output</span></code> API</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_isotonic_regression.html">Isotonic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_metadata_routing.html">Metadata Routing</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_multilabel.html">Multilabel classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_roc_curve_visualization_api.html">ROC Curve with Visualization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_johnson_lindenstrauss_bound.html">The Johnson-Lindenstrauss bound for embedding with random projections</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous/plot_display_object_visualization.html">Visualizations with Display Objects</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="impute/index.html">Missing Value Imputation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="impute/plot_missing_values.html">Imputing missing values before building an estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="impute/plot_iterative_imputer_variants_comparison.html">Imputing missing values with variants of IterativeImputer</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="model_selection/index.html">Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_grid_search_refit_callable.html">Balance model complexity and cross-validated score</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_likelihood_ratios.html">Class Likelihood Ratios to measure classification performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_randomized_search.html">Comparing randomized search and grid search for hyperparameter estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_successive_halving_heatmap.html">Comparison between grid search and successive halving</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_confusion_matrix.html">Confusion matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_grid_search_digits.html">Custom refit strategy of a grid search with cross-validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_multi_metric_evaluation.html">Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_det.html">Detection error tradeoff (DET) curve</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_roc.html">Multiclass Receiver Operating Characteristic (ROC)</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_nested_cross_validation_iris.html">Nested versus non-nested cross-validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_cv_predict.html">Plotting Cross-Validated Predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_learning_curve.html">Plotting Learning Curves and Checking Models’ Scalability</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_validation_curve.html">Plotting Validation Curves</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_tuned_decision_threshold.html">Post-hoc tuning the cut-off point of decision function</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_cost_sensitive_learning.html">Post-tuning the decision threshold for cost-sensitive learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_precision_recall.html">Precision-Recall</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_roc_crossval.html">Receiver Operating Characteristic (ROC) with cross validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_grid_search_text_feature_extraction.html">Sample pipeline for text feature extraction and evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_grid_search_stats.html">Statistical comparison of models using grid search</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_successive_halving_iterations.html">Successive Halving Iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_permutation_tests_for_classification.html">Test with permutations the significance of a classification score</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_train_error_vs_test_error.html">Train error vs Test error</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_underfitting_overfitting.html">Underfitting vs. Overfitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_selection/plot_cv_indices.html">Visualizing cross-validation behavior in scikit-learn</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="multiclass/index.html">Multiclass methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="multiclass/plot_multiclass_overview.html">Overview of multiclass training meta-estimators</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="multioutput/index.html">Multioutput methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="multioutput/plot_classifier_chain_yeast.html">Multilabel classification using a classifier chain</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="neighbors/index.html">Nearest Neighbors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neighbors/approximate_nearest_neighbors.html">Approximate nearest neighbors in TSNE</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_caching_nearest_neighbors.html">Caching nearest neighbors</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_nca_classification.html">Comparing Nearest Neighbors with and without Neighborhood Components Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_nca_dim_reduction.html">Dimensionality Reduction with Neighborhood Components Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_species_kde.html">Kernel Density Estimate of Species Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_digits_kde_sampling.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_nearest_centroid.html">Nearest Centroid Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_classification.html">Nearest Neighbors Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_regression.html">Nearest Neighbors regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_nca_illustration.html">Neighborhood Components Analysis Illustration</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_lof_novelty_detection.html">Novelty detection with Local Outlier Factor (LOF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_lof_outlier_detection.html">Outlier detection with Local Outlier Factor (LOF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbors/plot_kde_1d.html">Simple 1D Kernel Density Estimation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="neural_networks/index.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neural_networks/plot_mlp_training_curves.html">Compare Stochastic learning strategies for MLPClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_networks/plot_rbm_logistic_classification.html">Restricted Boltzmann Machine features for digit classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_networks/plot_mlp_alpha.html">Varying regularization in Multi-layer Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="neural_networks/plot_mnist_filters.html">Visualization of MLP weights on MNIST</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="compose/index.html">Pipelines and composite estimators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="compose/plot_column_transformer.html">Column Transformer with Heterogeneous Data Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="compose/plot_column_transformer_mixed_types.html">Column Transformer with Mixed Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="compose/plot_feature_union.html">Concatenating multiple feature extraction methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="compose/plot_transformed_target.html">Effect of transforming the targets in regression model</a></li>
<li class="toctree-l2"><a class="reference internal" href="compose/plot_digits_pipe.html">Pipelining: chaining a PCA and a logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="compose/plot_compare_reduction.html">Selecting dimensionality reduction with Pipeline and GridSearchCV</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="preprocessing/index.html">Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_all_scaling.html">Compare the effect of different scalers on data with outliers</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_target_encoder.html">Comparing Target Encoder with Other Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_discretization_strategies.html">Demonstrating the different strategies of KBinsDiscretizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_discretization_classification.html">Feature discretization</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_scaling_importance.html">Importance of Feature Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_map_data_to_normal.html">Map data to a normal distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_target_encoder_cross_val.html">Target Encoder’s Internal Cross fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="preprocessing/plot_discretization.html">Using KBinsDiscretizer to discretize continuous features</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="semi_supervised/index.html">Semi Supervised Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised/plot_semi_supervised_versus_svm_iris.html">Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised/plot_self_training_varying_threshold.html">Effect of varying threshold for self-training</a></li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised/plot_label_propagation_digits_active_learning.html">Label Propagation digits active learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised/plot_label_propagation_digits.html">Label Propagation digits: Demonstrating performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised/plot_label_propagation_structure.html">Label Propagation learning a complex structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="semi_supervised/plot_semi_supervised_newsgroups.html">Semi-supervised Classification on a Text Dataset</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="svm/index.html">Support Vector Machines</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_oneclass.html">One-class SVM with non-linear kernel (RBF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_svm_kernels.html">Plot classification boundaries with different SVM Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_iris_svc.html">Plot different SVM classifiers in the iris dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_linearsvc_support_vectors.html">Plot the support vectors in LinearSVC</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_rbf_parameters.html">RBF SVM parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_svm_margin.html">SVM Margins Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_svm_tie_breaking.html">SVM Tie Breaking Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_custom_kernel.html">SVM with custom kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_svm_anova.html">SVM-Anova: SVM with univariate feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_separating_hyperplane.html">SVM: Maximum margin separating hyperplane</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_separating_hyperplane_unbalanced.html">SVM: Separating hyperplane for unbalanced classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_weighted_samples.html">SVM: Weighted samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_svm_scale_c.html">Scaling the regularization parameter for SVCs</a></li>
<li class="toctree-l2"><a class="reference internal" href="svm/plot_svm_regression.html">Support Vector Regression (SVR) using linear and non-linear kernels</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="exercises/index.html">Tutorial exercises</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="exercises/plot_cv_diabetes.html">Cross-validation on diabetes Dataset Exercise</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/plot_digits_classification_exercise.html">Digits Classification Exercise</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/plot_iris_exercise.html">SVM Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="text/index.html">Working with text documents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="text/plot_document_classification_20newsgroups.html">Classification of text documents using sparse features</a></li>
<li class="toctree-l2"><a class="reference internal" href="text/plot_document_clustering.html">Clustering text documents using k-means</a></li>
<li class="toctree-l2"><a class="reference internal" href="text/plot_hashing_vs_dict_vectorizer.html">FeatureHasher and DictVectorizer Comparison</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Examples</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="examples">
<span id="general-examples"></span><h1>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h1>
<p>This is the gallery of examples that showcase how scikit-learn can be used. Some
examples demonstrate the use of the <a class="reference internal" href="../api/index.html#api-ref"><span class="std std-ref">API</span></a> in general and some
demonstrate specific applications in tutorial form. Also check out our
<a class="reference internal" href="../user_guide.html#user-guide"><span class="std std-ref">user guide</span></a> for more detailed illustrations.</p>
<div class="sphx-glr-thumbnails"></div><section id="release-highlights">
<h2>Release Highlights<a class="headerlink" href="#release-highlights" title="Link to this heading">#</a></h2>
<p>These examples illustrate the main features of the releases of scikit-learn.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 1.5! Many bug fixes and improvements were added, as well as some key new features. Below we detail the highlights of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_1_5&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_1_5_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_1_5_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-5-0-py"><span class="std std-ref">Release Highlights for scikit-learn 1.5</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 1.5</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 1.4! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_1_4&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_1_4_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_1_4_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-4-0-py"><span class="std std-ref">Release Highlights for scikit-learn 1.4</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 1.4</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 1.3! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_1_3&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_1_3_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_1_3_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-3-0-py"><span class="std std-ref">Release Highlights for scikit-learn 1.3</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 1.3</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 1.2! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_1_2&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_1_2_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_1_2_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-2-0-py"><span class="std std-ref">Release Highlights for scikit-learn 1.2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 1.2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 1.1! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_1_1&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_1_1_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_1_1_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-1-0-py"><span class="std std-ref">Release Highlights for scikit-learn 1.1</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 1.1</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are very pleased to announce the release of scikit-learn 1.0! The library has been stable for quite some time, releasing version 1.0 is recognizing that and signalling it to our users. This release does not include any breaking changes apart from the usual two-release deprecation cycle. For the future, we do our best to keep this pattern."><img alt="" src="../_images/sphx_glr_plot_release_highlights_1_0_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_1_0_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-0-0-py"><span class="std std-ref">Release Highlights for scikit-learn 1.0</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 1.0</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 0.24! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_0_24&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_0_24_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_0_24_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-24-0-py"><span class="std std-ref">Release Highlights for scikit-learn 0.24</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 0.24</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 0.23! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_0_23&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_0_23_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_0_23_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-23-0-py"><span class="std std-ref">Release Highlights for scikit-learn 0.23</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 0.23</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We are pleased to announce the release of scikit-learn 0.22, which comes with many bug fixes and new features! We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the release notes &lt;release_notes_0_22&gt;."><img alt="" src="../_images/sphx_glr_plot_release_highlights_0_22_0_thumb.png" />
<p><a class="reference internal" href="release_highlights/plot_release_highlights_0_22_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-22-0-py"><span class="std std-ref">Release Highlights for scikit-learn 0.22</span></a></p>
  <div class="sphx-glr-thumbnail-title">Release Highlights for scikit-learn 0.22</div>
</div></div></section>
<section id="biclustering">
<h2>Biclustering<a class="headerlink" href="#biclustering" title="Link to this heading">#</a></h2>
<p>Examples concerning biclustering techniques.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to generate a checkerboard dataset and bicluster it using the SpectralBiclustering algorithm. The spectral biclustering algorithm is specifically designed to cluster data by simultaneously considering both the rows (samples) and columns (features) of a matrix. It aims to identify patterns not only between samples but also within subsets of samples, allowing for the detection of localized structure within the data. This makes spectral biclustering particularly well-suited for datasets where the order or arrangement of features is fixed, such as in images, time series, or genomes."><img alt="" src="../_images/sphx_glr_plot_spectral_biclustering_thumb.png" />
<p><a class="reference internal" href="bicluster/plot_spectral_biclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-biclustering-py"><span class="std std-ref">A demo of the Spectral Biclustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">A demo of the Spectral Biclustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm."><img alt="" src="../_images/sphx_glr_plot_spectral_coclustering_thumb.png" />
<p><a class="reference internal" href="bicluster/plot_spectral_coclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-coclustering-py"><span class="std std-ref">A demo of the Spectral Co-Clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">A demo of the Spectral Co-Clustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The &#x27;comp.os.ms-windows.misc&#x27; category is excluded because it contains many posts containing nothing but data."><img alt="" src="../_images/sphx_glr_plot_bicluster_newsgroups_thumb.png" />
<p><a class="reference internal" href="bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py"><span class="std std-ref">Biclustering documents with the Spectral Co-clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">Biclustering documents with the Spectral Co-clustering algorithm</div>
</div></div></section>
<section id="calibration">
<h2>Calibration<a class="headerlink" href="#calibration" title="Link to this heading">#</a></h2>
<p>Examples illustrating the calibration of predicted probabilities of classifiers.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Well calibrated classifiers are probabilistic classifiers for which the output of predict_proba can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that for the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class."><img alt="" src="../_images/sphx_glr_plot_compare_calibration_thumb.png" />
<p><a class="reference internal" href="calibration/plot_compare_calibration.html#sphx-glr-auto-examples-calibration-plot-compare-calibration-py"><span class="std std-ref">Comparison of Calibration of Classifiers</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of Calibration of Classifiers</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to visualize how well calibrated the predicted probabilities are using calibration curves, also known as reliability diagrams. Calibration of an uncalibrated classifier will also be demonstrated."><img alt="" src="../_images/sphx_glr_plot_calibration_curve_thumb.png" />
<p><a class="reference internal" href="calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py"><span class="std std-ref">Probability Calibration curves</span></a></p>
  <div class="sphx-glr-thumbnail-title">Probability Calibration curves</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3)."><img alt="" src="../_images/sphx_glr_plot_calibration_multiclass_thumb.png" />
<p><a class="reference internal" href="calibration/plot_calibration_multiclass.html#sphx-glr-auto-examples-calibration-plot-calibration-multiclass-py"><span class="std std-ref">Probability Calibration for 3-class classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Probability Calibration for 3-class classification</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for this calibration and evaluates the quality of the returned probabilities using Brier&#x27;s score (see https://en.wikipedia.org/wiki/Brier_score)."><img alt="" src="../_images/sphx_glr_plot_calibration_thumb.png" />
<p><a class="reference internal" href="calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py"><span class="std std-ref">Probability calibration of classifiers</span></a></p>
  <div class="sphx-glr-thumbnail-title">Probability calibration of classifiers</div>
</div></div></section>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Link to this heading">#</a></h2>
<p>General examples about classification algorithms.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A comparison of several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets."><img alt="" src="../_images/sphx_glr_plot_classifier_comparison_thumb.png" />
<p><a class="reference internal" href="classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py"><span class="std std-ref">Classifier comparison</span></a></p>
  <div class="sphx-glr-thumbnail-title">Classifier comparison</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example plots the covariance ellipsoids of each class and the decision boundary learned by LinearDiscriminantAnalysis (LDA) and QuadraticDiscriminantAnalysis (QDA). The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA."><img alt="" src="../_images/sphx_glr_plot_lda_qda_thumb.png" />
<p><a class="reference internal" href="classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py"><span class="std std-ref">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</span></a></p>
  <div class="sphx-glr-thumbnail-title">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how the Ledoit-Wolf and Oracle Approximating Shrinkage (OAS) estimators of covariance can improve classification."><img alt="" src="../_images/sphx_glr_plot_lda_thumb.png" />
<p><a class="reference internal" href="classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py"><span class="std std-ref">Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, L1 and L2 penalized logistic regression (multinomial multiclass), a One-Vs-Rest version with logistic regression, and Gaussian process classification."><img alt="" src="../_images/sphx_glr_plot_classification_probability_thumb.png" />
<p><a class="reference internal" href="classification/plot_classification_probability.html#sphx-glr-auto-examples-classification-plot-classification-probability-py"><span class="std std-ref">Plot classification probability</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot classification probability</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how scikit-learn can be used to recognize images of hand-written digits, from 0-9."><img alt="" src="../_images/sphx_glr_plot_digits_classification_thumb.png" />
<p><a class="reference internal" href="classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="std std-ref">Recognizing hand-written digits</span></a></p>
  <div class="sphx-glr-thumbnail-title">Recognizing hand-written digits</div>
</div></div></section>
<section id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.cluster.html#module-sklearn.cluster" title="sklearn.cluster"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.cluster</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results."><img alt="" src="../_images/sphx_glr_plot_kmeans_digits_thumb.png" />
<p><a class="reference internal" href="cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"><span class="std std-ref">A demo of K-Means clustering on the handwritten digits data</span></a></p>
  <div class="sphx-glr-thumbnail-title">A demo of K-Means clustering on the handwritten digits data</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Compute the segmentation of a 2D image with Ward hierarchical clustering. The clustering is spatially constrained in order for each segmented region to be in one piece."><img alt="" src="../_images/sphx_glr_plot_coin_ward_segmentation_thumb.png" />
<p><a class="reference internal" href="cluster/plot_coin_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py"><span class="std std-ref">A demo of structured Ward hierarchical clustering on an image of coins</span></a></p>
  <div class="sphx-glr-thumbnail-title">A demo of structured Ward hierarchical clustering on an image of coins</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Reference:"><img alt="" src="../_images/sphx_glr_plot_mean_shift_thumb.png" />
<p><a class="reference internal" href="cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py"><span class="std std-ref">A demo of the mean-shift clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">A demo of the mean-shift clustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="- a first experiment with fixed &quot;ground truth labels&quot; (and therefore fixed   number of classes) and randomly &quot;predicted labels&quot;; - a second experiment with varying &quot;ground truth labels&quot;, randomly &quot;predicted   labels&quot;. The &quot;predicted labels&quot; have the same number of classes and clusters   as the &quot;ground truth labels&quot;."><img alt="" src="../_images/sphx_glr_plot_adjusted_for_chance_measures_thumb.png" />
<p><a class="reference internal" href="cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="std std-ref">Adjustment for chance in clustering performance evaluation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Adjustment for chance in clustering performance evaluation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors."><img alt="" src="../_images/sphx_glr_plot_agglomerative_clustering_thumb.png" />
<p><a class="reference internal" href="cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py"><span class="std std-ref">Agglomerative clustering with and without structure</span></a></p>
  <div class="sphx-glr-thumbnail-title">Agglomerative clustering with and without structure</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Demonstrates the effect of different metrics on the hierarchical clustering."><img alt="" src="../_images/sphx_glr_plot_agglomerative_clustering_metrics_thumb.png" />
<p><a class="reference internal" href="cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py"><span class="std std-ref">Agglomerative clustering with different metrics</span></a></p>
  <div class="sphx-glr-thumbnail-title">Agglomerative clustering with different metrics</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example to show the output of the sklearn.cluster.kmeans_plusplus function for generating initial seeds for clustering."><img alt="" src="../_images/sphx_glr_plot_kmeans_plusplus_thumb.png" />
<p><a class="reference internal" href="cluster/plot_kmeans_plusplus.html#sphx-glr-auto-examples-cluster-plot-kmeans-plusplus-py"><span class="std std-ref">An example of K-Means++ initialization</span></a></p>
  <div class="sphx-glr-thumbnail-title">An example of K-Means++ initialization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows differences between Regular K-Means algorithm and Bisecting K-Means."><img alt="" src="../_images/sphx_glr_plot_bisect_kmeans_thumb.png" />
<p><a class="reference internal" href="cluster/plot_bisect_kmeans.html#sphx-glr-auto-examples-cluster-plot-bisect-kmeans-py"><span class="std std-ref">Bisecting K-Means and Regular K-Means Performance Comparison</span></a></p>
  <div class="sphx-glr-thumbnail-title">Bisecting K-Means and Regular K-Means Performance Comparison</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Performs a pixel-wise Vector Quantization (VQ) of an image of the summer palace (China), reducing the number of colors required to show the image from 96,615 unique colors to 64, while preserving the overall appearance quality."><img alt="" src="../_images/sphx_glr_plot_color_quantization_thumb.png" />
<p><a class="reference internal" href="cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py"><span class="std std-ref">Color Quantization using K-Means</span></a></p>
  <div class="sphx-glr-thumbnail-title">Color Quantization using K-Means</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares the timing of BIRCH (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 25,000 samples and 2 features generated using make_blobs."><img alt="" src="../_images/sphx_glr_plot_birch_vs_minibatchkmeans_thumb.png" />
<p><a class="reference internal" href="cluster/plot_birch_vs_minibatchkmeans.html#sphx-glr-auto-examples-cluster-plot-birch-vs-minibatchkmeans-py"><span class="std std-ref">Compare BIRCH and MiniBatchKMeans</span></a></p>
  <div class="sphx-glr-thumbnail-title">Compare BIRCH and MiniBatchKMeans</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows characteristics of different clustering algorithms on datasets that are &quot;interesting&quot; but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others."><img alt="" src="../_images/sphx_glr_plot_cluster_comparison_thumb.png" />
<p><a class="reference internal" href="cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py"><span class="std std-ref">Comparing different clustering algorithms on toy datasets</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing different clustering algorithms on toy datasets</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows characteristics of different linkage methods for hierarchical clustering on datasets that are &quot;interesting&quot; but still in 2D."><img alt="" src="../_images/sphx_glr_plot_linkage_comparison_thumb.png" />
<p><a class="reference internal" href="cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py"><span class="std std-ref">Comparing different hierarchical linkage methods on toy datasets</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing different hierarchical linkage methods on toy datasets</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but gives slightly different results (see mini_batch_kmeans)."><img alt="" src="../_images/sphx_glr_plot_mini_batch_kmeans_thumb.png" />
<p><a class="reference internal" href="cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py"><span class="std std-ref">Comparison of the K-Means and MiniBatchKMeans clustering algorithms</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of the K-Means and MiniBatchKMeans clustering algorithms</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds core samples in regions of high density and expands clusters from them. This algorithm is good for data which contains clusters of similar density."><img alt="" src="../_images/sphx_glr_plot_dbscan_thumb.png" />
<p><a class="reference internal" href="cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"><span class="std std-ref">Demo of DBSCAN clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demo of DBSCAN clustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this demo we will take a look at cluster.HDBSCAN from the perspective of generalizing the cluster.DBSCAN algorithm. We&#x27;ll compare both algorithms on specific datasets. Finally we&#x27;ll evaluate HDBSCAN&#x27;s sensitivity to certain hyperparameters."><img alt="" src="../_images/sphx_glr_plot_hdbscan_thumb.png" />
<p><a class="reference internal" href="cluster/plot_hdbscan.html#sphx-glr-auto-examples-cluster-plot-hdbscan-py"><span class="std std-ref">Demo of HDBSCAN clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demo of HDBSCAN clustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Finds core samples of high density and expands clusters from them. This example uses data that is generated so that the clusters have different densities."><img alt="" src="../_images/sphx_glr_plot_optics_thumb.png" />
<p><a class="reference internal" href="cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py"><span class="std std-ref">Demo of OPTICS clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demo of OPTICS clustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Reference: Brendan J. Frey and Delbert Dueck, &quot;Clustering by Passing Messages Between Data Points&quot;, Science Feb. 2007"><img alt="" src="../_images/sphx_glr_plot_affinity_propagation_thumb.png" />
<p><a class="reference internal" href="cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py"><span class="std std-ref">Demo of affinity propagation clustering algorithm</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demo of affinity propagation clustering algorithm</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example is meant to illustrate situations where k-means produces unintuitive and possibly undesirable clusters."><img alt="" src="../_images/sphx_glr_plot_kmeans_assumptions_thumb.png" />
<p><a class="reference internal" href="cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py"><span class="std std-ref">Demonstration of k-means assumptions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demonstration of k-means assumptions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Evaluate the ability of k-means initializations strategies to make the algorithm convergence robust, as measured by the relative standard deviation of the inertia of the clustering (i.e. the sum of squared distances to the nearest cluster center)."><img alt="" src="../_images/sphx_glr_plot_kmeans_stability_low_dim_dense_thumb.png" />
<p><a class="reference internal" href="cluster/plot_kmeans_stability_low_dim_dense.html#sphx-glr-auto-examples-cluster-plot-kmeans-stability-low-dim-dense-py"><span class="std std-ref">Empirical evaluation of the impact of k-means initialization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Empirical evaluation of the impact of k-means initialization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="These images show how similar features are merged together using feature agglomeration."><img alt="" src="../_images/sphx_glr_plot_digits_agglomeration_thumb.png" />
<p><a class="reference internal" href="cluster/plot_digits_agglomeration.html#sphx-glr-auto-examples-cluster-plot-digits-agglomeration-py"><span class="std std-ref">Feature agglomeration</span></a></p>
  <div class="sphx-glr-thumbnail-title">Feature agglomeration</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares 2 dimensionality reduction strategies:"><img alt="" src="../_images/sphx_glr_plot_feature_agglomeration_vs_univariate_selection_thumb.png" />
<p><a class="reference internal" href="cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="std std-ref">Feature agglomeration vs. univariate selection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Feature agglomeration vs. univariate selection</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Example builds a swiss roll dataset and runs hierarchical clustering on their position."><img alt="" src="../_images/sphx_glr_plot_ward_structured_vs_unstructured_thumb.png" />
<p><a class="reference internal" href="cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py"><span class="std std-ref">Hierarchical clustering: structured vs unstructured ward</span></a></p>
  <div class="sphx-glr-thumbnail-title">Hierarchical clustering: structured vs unstructured ward</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Clustering can be expensive, especially when our dataset contains millions of datapoints. Many clustering algorithms are not inductive and so cannot be directly applied to new data samples without recomputing the clustering, which may be intractable. Instead, we can use clustering to then learn an inductive model with a classifier, which has several benefits:"><img alt="" src="../_images/sphx_glr_plot_inductive_clustering_thumb.png" />
<p><a class="reference internal" href="cluster/plot_inductive_clustering.html#sphx-glr-auto-examples-cluster-plot-inductive-clustering-py"><span class="std std-ref">Inductive Clustering</span></a></p>
  <div class="sphx-glr-thumbnail-title">Inductive Clustering</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The plot shows:"><img alt="" src="../_images/sphx_glr_plot_cluster_iris_thumb.png" />
<p><a class="reference internal" href="cluster/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py"><span class="std std-ref">K-means Clustering</span></a></p>
  <div class="sphx-glr-thumbnail-title">K-means Clustering</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces."><img alt="" src="../_images/sphx_glr_plot_dict_face_patches_thumb.png" />
<p><a class="reference internal" href="cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="std std-ref">Online learning of a dictionary of parts of faces</span></a></p>
  <div class="sphx-glr-thumbnail-title">Online learning of a dictionary of parts of faces</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot Hierarchical Clustering Dendrogram"><img alt="" src="../_images/sphx_glr_plot_agglomerative_dendrogram_thumb.png" />
<p><a class="reference internal" href="cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py"><span class="std std-ref">Plot Hierarchical Clustering Dendrogram</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot Hierarchical Clustering Dendrogram</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example uses spectral_clustering on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions."><img alt="" src="../_images/sphx_glr_plot_coin_segmentation_thumb.png" />
<p><a class="reference internal" href="cluster/plot_coin_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-segmentation-py"><span class="std std-ref">Segmenting the picture of greek coins in regions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Segmenting the picture of greek coins in regions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1]."><img alt="" src="../_images/sphx_glr_plot_kmeans_silhouette_analysis_thumb.png" />
<p><a class="reference internal" href="cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"><span class="std std-ref">Selecting the number of clusters with silhouette analysis on KMeans clustering</span></a></p>
  <div class="sphx-glr-thumbnail-title">Selecting the number of clusters with silhouette analysis on KMeans clustering</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, an image with connected circles is generated and spectral clustering is used to separate the circles."><img alt="" src="../_images/sphx_glr_plot_segmentation_toy_thumb.png" />
<p><a class="reference internal" href="cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py"><span class="std std-ref">Spectral clustering for image segmentation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Spectral clustering for image segmentation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An illustration of various linkage option for agglomerative clustering on a 2D embedding of the digits dataset."><img alt="" src="../_images/sphx_glr_plot_digits_linkage_thumb.png" />
<p><a class="reference internal" href="cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py"><span class="std std-ref">Various Agglomerative Clustering on a 2D embedding of digits</span></a></p>
  <div class="sphx-glr-thumbnail-title">Various Agglomerative Clustering on a 2D embedding of digits</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how one can use KBinsDiscretizer to perform vector quantization on a set of toy image, the raccoon face."><img alt="" src="../_images/sphx_glr_plot_face_compress_thumb.png" />
<p><a class="reference internal" href="cluster/plot_face_compress.html#sphx-glr-auto-examples-cluster-plot-face-compress-py"><span class="std std-ref">Vector Quantization Example</span></a></p>
  <div class="sphx-glr-thumbnail-title">Vector Quantization Example</div>
</div></div></section>
<section id="covariance-estimation">
<h2>Covariance estimation<a class="headerlink" href="#covariance-estimation" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.covariance.html#module-sklearn.covariance" title="sklearn.covariance"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.covariance</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate."><img alt="" src="../_images/sphx_glr_plot_lw_vs_oas_thumb.png" />
<p><a class="reference internal" href="covariance/plot_lw_vs_oas.html#sphx-glr-auto-examples-covariance-plot-lw-vs-oas-py"><span class="std std-ref">Ledoit-Wolf vs OAS estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Ledoit-Wolf vs OAS estimation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows covariance estimation with Mahalanobis distances on Gaussian distributed data."><img alt="" src="../_images/sphx_glr_plot_mahalanobis_distances_thumb.png" />
<p><a class="reference internal" href="covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py"><span class="std std-ref">Robust covariance estimation and Mahalanobis distances relevance</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust covariance estimation and Mahalanobis distances relevance</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &quot;erroneous&quot; observations in the data set. [1]_, [2]_"><img alt="" src="../_images/sphx_glr_plot_robust_vs_empirical_covariance_thumb.png" />
<p><a class="reference internal" href="covariance/plot_robust_vs_empirical_covariance.html#sphx-glr-auto-examples-covariance-plot-robust-vs-empirical-covariance-py"><span class="std std-ref">Robust vs Empirical covariance estimate</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust vs Empirical covariance estimate</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in shrunk_covariance estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off."><img alt="" src="../_images/sphx_glr_plot_covariance_estimation_thumb.png" />
<p><a class="reference internal" href="covariance/plot_covariance_estimation.html#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py"><span class="std std-ref">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</span></a></p>
  <div class="sphx-glr-thumbnail-title">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Using the GraphicalLasso estimator to learn a covariance and sparse precision from a small number of samples."><img alt="" src="../_images/sphx_glr_plot_sparse_cov_thumb.png" />
<p><a class="reference internal" href="covariance/plot_sparse_cov.html#sphx-glr-auto-examples-covariance-plot-sparse-cov-py"><span class="std std-ref">Sparse inverse covariance estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Sparse inverse covariance estimation</div>
</div></div></section>
<section id="cross-decomposition">
<h2>Cross decomposition<a class="headerlink" href="#cross-decomposition" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.cross_decomposition.html#module-sklearn.cross_decomposition" title="sklearn.cross_decomposition"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.cross_decomposition</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Simple usage of various cross decomposition algorithms:"><img alt="" src="../_images/sphx_glr_plot_compare_cross_decomposition_thumb.png" />
<p><a class="reference internal" href="cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py"><span class="std std-ref">Compare cross decomposition methods</span></a></p>
  <div class="sphx-glr-thumbnail-title">Compare cross decomposition methods</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares Principal Component Regression (PCR) and Partial Least Squares Regression (PLS) on a toy dataset. Our goal is to illustrate how PLS can outperform PCR when the target is strongly correlated with some directions in the data that have a low variance."><img alt="" src="../_images/sphx_glr_plot_pcr_vs_pls_thumb.png" />
<p><a class="reference internal" href="cross_decomposition/plot_pcr_vs_pls.html#sphx-glr-auto-examples-cross-decomposition-plot-pcr-vs-pls-py"><span class="std std-ref">Principal Component Regression vs Partial Least Squares Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Principal Component Regression vs Partial Least Squares Regression</div>
</div></div></section>
<section id="dataset-examples">
<h2>Dataset examples<a class="headerlink" href="#dataset-examples" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.datasets.html#module-sklearn.datasets" title="sklearn.datasets"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.datasets</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example plots several randomly generated classification datasets. For easy visualization, all datasets have 2 features, plotted on the x and y axis. The color of each point represents its class label."><img alt="" src="../_images/sphx_glr_plot_random_dataset_thumb.png" />
<p><a class="reference internal" href="datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py"><span class="std std-ref">Plot randomly generated classification dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot randomly generated classification dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This illustrates the make_multilabel_classification dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes."><img alt="" src="../_images/sphx_glr_plot_random_multilabel_dataset_thumb.png" />
<p><a class="reference internal" href="datasets/plot_random_multilabel_dataset.html#sphx-glr-auto-examples-datasets-plot-random-multilabel-dataset-py"><span class="std std-ref">Plot randomly generated multilabel dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot randomly generated multilabel dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we&#x27;d have to first transform it into a feature vector with length 64."><img alt="" src="../_images/sphx_glr_plot_digits_last_image_thumb.png" />
<p><a class="reference internal" href="datasets/plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py"><span class="std std-ref">The Digit Dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">The Digit Dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width."><img alt="" src="../_images/sphx_glr_plot_iris_dataset_thumb.png" />
<p><a class="reference internal" href="datasets/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py"><span class="std std-ref">The Iris Dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">The Iris Dataset</div>
</div></div></section>
<section id="decision-trees">
<h2>Decision Trees<a class="headerlink" href="#decision-trees" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.tree.html#module-sklearn.tree" title="sklearn.tree"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.tree</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A 1D regression with decision tree."><img alt="" src="../_images/sphx_glr_plot_tree_regression_thumb.png" />
<p><a class="reference internal" href="tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py"><span class="std std-ref">Decision Tree Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Decision Tree Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example to illustrate multi-output regression with decision tree."><img alt="" src="../_images/sphx_glr_plot_tree_regression_multioutput_thumb.png" />
<p><a class="reference internal" href="tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py"><span class="std std-ref">Multi-output Decision Tree Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multi-output Decision Tree Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the decision surface of a decision tree trained on pairs of features of the iris dataset."><img alt="" src="../_images/sphx_glr_plot_iris_dtc_thumb.png" />
<p><a class="reference internal" href="tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py"><span class="std std-ref">Plot the decision surface of decision trees trained on the iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot the decision surface of decision trees trained on the iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The DecisionTreeClassifier provides parameters such as min_samples_leaf and max_depth to prevent a tree from overfiting. Cost complexity pruning provides another option to control the size of a tree. In DecisionTreeClassifier, this pruning technique is parameterized by the cost complexity parameter, ccp_alpha. Greater values of ccp_alpha increase the number of nodes pruned. Here we only show the effect of ccp_alpha on regularizing the trees and how to choose a ccp_alpha based on validation scores."><img alt="" src="../_images/sphx_glr_plot_cost_complexity_pruning_thumb.png" />
<p><a class="reference internal" href="tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py"><span class="std std-ref">Post pruning decision trees with cost complexity pruning</span></a></p>
  <div class="sphx-glr-thumbnail-title">Post pruning decision trees with cost complexity pruning</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:"><img alt="" src="../_images/sphx_glr_plot_unveil_tree_structure_thumb.png" />
<p><a class="reference internal" href="tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"><span class="std std-ref">Understanding the decision tree structure</span></a></p>
  <div class="sphx-glr-thumbnail-title">Understanding the decision tree structure</div>
</div></div></section>
<section id="decomposition">
<h2>Decomposition<a class="headerlink" href="#decomposition" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.decomposition.html#module-sklearn.decomposition" title="sklearn.decomposition"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.decomposition</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="An example of estimating sources from noisy data."><img alt="" src="../_images/sphx_glr_plot_ica_blind_source_separation_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py"><span class="std std-ref">Blind source separation using FastICA</span></a></p>
  <div class="sphx-glr-thumbnail-title">Blind source separation using FastICA</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width."><img alt="" src="../_images/sphx_glr_plot_pca_vs_lda_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of LDA and PCA 2D projection of Iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example applies to olivetti_faces_dataset different unsupervised matrix decomposition (dimension reduction) methods from the module sklearn.decomposition (see the documentation chapter decompositions)."><img alt="" src="../_images/sphx_glr_plot_faces_decomposition_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Faces dataset decompositions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Investigating the Iris dataset, we see that sepal length, petal length and petal width are highly correlated. Sepal width is less redundant. Matrix decomposition techniques can uncover these latent patterns. Applying rotations to the resulting components does not inherently improve the predictive value of the derived latent space, but can help visualise their structure; here, for example, the varimax rotation, which is found by maximizing the squared variances of the weights, finds a structure where the second component only loads positively on sepal width."><img alt="" src="../_images/sphx_glr_plot_varimax_fa_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_varimax_fa.html#sphx-glr-auto-examples-decomposition-plot-varimax-fa-py"><span class="std std-ref">Factor Analysis (with rotation) to visualize patterns</span></a></p>
  <div class="sphx-glr-thumbnail-title">Factor Analysis (with rotation) to visualize patterns</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates visually in the feature space a comparison by results using two different component analysis techniques."><img alt="" src="../_images/sphx_glr_plot_ica_vs_pca_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py"><span class="std std-ref">FastICA on 2D point clouds</span></a></p>
  <div class="sphx-glr-thumbnail-title">FastICA on 2D point clouds</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example comparing the effect of reconstructing noisy fragments of a raccoon face image using firstly online DictionaryLearning and various transform methods."><img alt="" src="../_images/sphx_glr_plot_image_denoising_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py"><span class="std std-ref">Image denoising using dictionary learning</span></a></p>
  <div class="sphx-glr-thumbnail-title">Image denoising using dictionary learning</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage."><img alt="" src="../_images/sphx_glr_plot_incremental_pca_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py"><span class="std std-ref">Incremental PCA</span></a></p>
  <div class="sphx-glr-thumbnail-title">Incremental PCA</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows the difference between the Principal Components Analysis (~sklearn.decomposition.PCA) and its kernelized version (~sklearn.decomposition.KernelPCA)."><img alt="" src="../_images/sphx_glr_plot_kernel_pca_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py"><span class="std std-ref">Kernel PCA</span></a></p>
  <div class="sphx-glr-thumbnail-title">Kernel PCA</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Probabilistic PCA and Factor Analysis are probabilistic models. The consequence is that the likelihood of new data can be used for model selection and covariance estimation. Here we compare PCA and FA with cross-validation on low rank data corrupted with homoscedastic noise (noise variance is the same for each feature) or heteroscedastic noise (noise variance is the different for each feature). In a second step we compare the model likelihood to the likelihoods obtained from shrinkage covariance estimators."><img alt="" src="../_images/sphx_glr_plot_pca_vs_fa_model_selection_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Model selection with Probabilistic PCA and Factor Analysis (FA)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Principal Component Analysis applied to the Iris dataset."><img alt="" src="../_images/sphx_glr_plot_pca_iris_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py"><span class="std std-ref">PCA example with Iris Data-set</span></a></p>
  <div class="sphx-glr-thumbnail-title">PCA example with Iris Data-set</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the SparseCoder estimator. The Ricker (also known as Mexican hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates learning the dictionary to best fit your type of signals."><img alt="" src="../_images/sphx_glr_plot_sparse_coding_thumb.png" />
<p><a class="reference internal" href="decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py"><span class="std std-ref">Sparse coding with a precomputed dictionary</span></a></p>
  <div class="sphx-glr-thumbnail-title">Sparse coding with a precomputed dictionary</div>
</div></div></section>
<section id="developing-estimators">
<h2>Developing Estimators<a class="headerlink" href="#developing-estimators" title="Link to this heading">#</a></h2>
<p>Examples concerning the development of Custom Estimator.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The __sklearn_is_fitted__ method is a convention used in scikit-learn for checking whether an estimator object has been fitted or not. This method is typically implemented in custom estimator classes that are built on top of scikit-learn&#x27;s base classes like BaseEstimator or its subclasses."><img alt="" src="../_images/sphx_glr_sklearn_is_fitted_thumb.png" />
<p><a class="reference internal" href="developing_estimators/sklearn_is_fitted.html#sphx-glr-auto-examples-developing-estimators-sklearn-is-fitted-py"><span class="std std-ref">__sklearn_is_fitted__ as Developer API</span></a></p>
  <div class="sphx-glr-thumbnail-title">__sklearn_is_fitted__ as Developer API</div>
</div></div></section>
<section id="ensemble-methods">
<h2>Ensemble methods<a class="headerlink" href="#ensemble-methods" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.ensemble.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this example, we will compare the training times and prediction performances of HistGradientBoostingRegressor with different encoding strategies for categorical features. In particular, we will evaluate:"><img alt="" src="../_images/sphx_glr_plot_gradient_boosting_categorical_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_gradient_boosting_categorical.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-categorical-py"><span class="std std-ref">Categorical Feature Support in Gradient Boosting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Categorical Feature Support in Gradient Boosting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Stacking refers to a method to blend estimators. In this strategy, some estimators are individually fitted on some training data while a final estimator is trained using the stacked predictions of these base estimators."><img alt="" src="../_images/sphx_glr_plot_stack_predictors_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py"><span class="std std-ref">Combine predictors using stacking</span></a></p>
  <div class="sphx-glr-thumbnail-title">Combine predictors using stacking</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example we compare the performance of Random Forest (RF) and Histogram Gradient Boosting (HGBT) models in terms of score and computation time for a regression dataset, though all the concepts here presented apply to classification as well."><img alt="" src="../_images/sphx_glr_plot_forest_hist_grad_boosting_comparison_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_forest_hist_grad_boosting_comparison.html#sphx-glr-auto-examples-ensemble-plot-forest-hist-grad-boosting-comparison-py"><span class="std std-ref">Comparing Random Forests and Histogram Gradient Boosting models</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing Random Forests and Histogram Gradient Boosting models</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example to compare multi-output regression with random forest and the multiclass meta-estimator."><img alt="" src="../_images/sphx_glr_plot_random_forest_regression_multioutput_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_random_forest_regression_multioutput.html#sphx-glr-auto-examples-ensemble-plot-random-forest-regression-multioutput-py"><span class="std std-ref">Comparing random forests and the multi-output meta estimator</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing random forests and the multi-output meta estimator</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail."><img alt="" src="../_images/sphx_glr_plot_adaboost_regression_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"><span class="std std-ref">Decision Tree Regression with AdaBoost</span></a></p>
  <div class="sphx-glr-thumbnail-title">Decision Tree Regression with AdaBoost</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Gradient Boosting is an ensemble technique that combines multiple weak learners, typically decision trees, to create a robust and powerful predictive model. It does so in an iterative fashion, where each new stage (tree) corrects the errors of the previous ones."><img alt="" src="../_images/sphx_glr_plot_gradient_boosting_early_stopping_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_gradient_boosting_early_stopping.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-early-stopping-py"><span class="std std-ref">Early stopping in Gradient Boosting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Early stopping in Gradient Boosting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows the use of a forest of trees to evaluate the importance of features on an artificial classification task. The blue bars are the feature importances of the forest, along with their inter-trees variability represented by the error bars."><img alt="" src="../_images/sphx_glr_plot_forest_importances_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"><span class="std std-ref">Feature importances with a forest of trees</span></a></p>
  <div class="sphx-glr-thumbnail-title">Feature importances with a forest of trees</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Transform your features into a higher dimensional, sparse space. Then train a linear model on these features."><img alt="" src="../_images/sphx_glr_plot_feature_transformation_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"><span class="std std-ref">Feature transformations with ensembles of trees</span></a></p>
  <div class="sphx-glr-thumbnail-title">Feature transformations with ensembles of trees</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="histogram_based_gradient_boosting (HGBT) models may be one of the most useful supervised learning models in scikit-learn. They are based on a modern gradient boosting implementation comparable to LightGBM and XGBoost. As such, HGBT models are more feature rich than and often outperform alternative models like random forests, especially when the number of samples is larger than some ten thousands (see sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py)."><img alt="" src="../_images/sphx_glr_plot_hgbt_regression_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_hgbt_regression.html#sphx-glr-auto-examples-ensemble-plot-hgbt-regression-py"><span class="std std-ref">Features in Histogram Gradient Boosting Trees</span></a></p>
  <div class="sphx-glr-thumbnail-title">Features in Histogram Gradient Boosting Trees</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Gradient Boosting Out-of-Bag estimates"><img alt="" src="../_images/sphx_glr_plot_gradient_boosting_oob_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Gradient Boosting Out-of-Bag estimates</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gradient Boosting Out-of-Bag estimates</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates Gradient Boosting to produce a predictive model from an ensemble of weak predictive models. Gradient boosting can be used for regression and classification problems. Here, we will train a model to tackle a diabetes regression task. We will obtain the results from GradientBoostingRegressor with least squares loss and 500 regression trees of depth 4."><img alt="" src="../_images/sphx_glr_plot_gradient_boosting_regression_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gradient Boosting regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 [1]_."><img alt="" src="../_images/sphx_glr_plot_gradient_boosting_regularization_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_gradient_boosting_regularization.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regularization-py"><span class="std std-ref">Gradient Boosting regularization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gradient Boosting regularization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might be beneficial for classification. The mapping is completely unsupervised and very efficient."><img alt="" src="../_images/sphx_glr_plot_random_forest_embedding_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py"><span class="std std-ref">Hashing feature transformation using Totally Random Trees</span></a></p>
  <div class="sphx-glr-thumbnail-title">Hashing feature transformation using Totally Random Trees</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example using IsolationForest for anomaly detection."><img alt="" src="../_images/sphx_glr_plot_isolation_forest_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_isolation_forest.html#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py"><span class="std std-ref">IsolationForest example</span></a></p>
  <div class="sphx-glr-thumbnail-title">IsolationForest example</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the effect of monotonic constraints on a gradient boosting estimator."><img alt="" src="../_images/sphx_glr_plot_monotonic_constraints_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_monotonic_constraints.html#sphx-glr-auto-examples-ensemble-plot-monotonic-constraints-py"><span class="std std-ref">Monotonic Constraints</span></a></p>
  <div class="sphx-glr-thumbnail-title">Monotonic Constraints</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how boosting can improve the prediction accuracy on a multi-label classification problem. It reproduces a similar experiment as depicted by Figure 1 in Zhu et al [1]_."><img alt="" src="../_images/sphx_glr_plot_adaboost_multiclass_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py"><span class="std std-ref">Multi-class AdaBoosted Decision Trees</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multi-class AdaBoosted Decision Trees</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations z_i = (x_i, y_i). The out-of-bag (OOB) error is the average error for each z_i calculated using predictions from the trees that do not contain z_i in their respective bootstrap sample. This allows the RandomForestClassifier to be fit and validated whilst being trained [1]_."><img alt="" src="../_images/sphx_glr_plot_ensemble_oob_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py"><span class="std std-ref">OOB Errors for Random Forests</span></a></p>
  <div class="sphx-glr-thumbnail-title">OOB Errors for Random Forests</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows the use of a forest of trees to evaluate the impurity based importance of the pixels in an image classification task on the faces dataset. The hotter the pixel, the more important it is."><img alt="" src="../_images/sphx_glr_plot_forest_importances_faces_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Pixel importances with a parallel forest of trees</span></a></p>
  <div class="sphx-glr-thumbnail-title">Pixel importances with a parallel forest of trees</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier."><img alt="" src="../_images/sphx_glr_plot_voting_probas_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py"><span class="std std-ref">Plot class probabilities calculated by the VotingClassifier</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot class probabilities calculated by the VotingClassifier</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. We will use three different regressors to predict the data: GradientBoostingRegressor, RandomForestRegressor, and LinearRegression). Then the above 3 regressors will be used for the VotingRegressor."><img alt="" src="../_images/sphx_glr_plot_voting_regressor_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py"><span class="std std-ref">Plot individual and voting regression predictions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot individual and voting regression predictions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the decision boundaries of a VotingClassifier for two features of the Iris dataset."><img alt="" src="../_images/sphx_glr_plot_voting_decision_regions_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py"><span class="std std-ref">Plot the decision boundaries of a VotingClassifier</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot the decision boundaries of a VotingClassifier</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset."><img alt="" src="../_images/sphx_glr_plot_forest_iris_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py"><span class="std std-ref">Plot the decision surfaces of ensembles of trees on the iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot the decision surfaces of ensembles of trees on the iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how quantile regression can be used to create prediction intervals. See sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py for an example showcasing some other features of HistGradientBoostingRegressor."><img alt="" src="../_images/sphx_glr_plot_gradient_boosting_quantile_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"><span class="std std-ref">Prediction Intervals for Gradient Boosting Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Prediction Intervals for Gradient Boosting Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble."><img alt="" src="../_images/sphx_glr_plot_bias_variance_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py"><span class="std std-ref">Single estimator versus bagging: bias-variance decomposition</span></a></p>
  <div class="sphx-glr-thumbnail-title">Single estimator versus bagging: bias-variance decomposition</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two &quot;Gaussian quantiles&quot; clusters (see sklearn.datasets.make_gaussian_quantiles) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value."><img alt="" src="../_images/sphx_glr_plot_adaboost_twoclass_thumb.png" />
<p><a class="reference internal" href="ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py"><span class="std std-ref">Two-class AdaBoost</span></a></p>
  <div class="sphx-glr-thumbnail-title">Two-class AdaBoost</div>
</div></div></section>
<section id="examples-based-on-real-world-datasets">
<h2>Examples based on real world datasets<a class="headerlink" href="#examples-based-on-real-world-datasets" title="Link to this heading">#</a></h2>
<p>Applications to real world problems with some medium sized datasets or
interactive user interface.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in computed tomography (CT)."><img alt="" src="../_images/sphx_glr_plot_tomography_l1_reconstruction_thumb.png" />
<p><a class="reference internal" href="applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="std std-ref">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The dataset used in this example is a preprocessed excerpt of the &quot;Labeled Faces in the Wild&quot;, aka LFW_:"><img alt="" src="../_images/sphx_glr_plot_face_recognition_thumb.png" />
<p><a class="reference internal" href="applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="std std-ref">Faces recognition example using eigenfaces and SVMs</span></a></p>
  <div class="sphx-glr-thumbnail-title">Faces recognition example using eigenfaces and SVMs</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to use KernelPCA to denoise images. In short, we take advantage of the approximation function learned during fit to reconstruct the original image."><img alt="" src="../_images/sphx_glr_plot_digits_denoising_thumb.png" />
<p><a class="reference internal" href="applications/plot_digits_denoising.html#sphx-glr-auto-examples-applications-plot-digits-denoising-py"><span class="std std-ref">Image denoising using kernel PCA</span></a></p>
  <div class="sphx-glr-thumbnail-title">Image denoising using kernel PCA</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how Polars-engineered lagged features can be used for time series forecasting with HistGradientBoostingRegressor on the Bike Sharing Demand dataset."><img alt="" src="../_images/sphx_glr_plot_time_series_lagged_features_thumb.png" />
<p><a class="reference internal" href="applications/plot_time_series_lagged_features.html#sphx-glr-auto-examples-applications-plot-time-series-lagged-features-py"><span class="std std-ref">Lagged features for time series forecasting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Lagged features for time series forecasting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Demonstrate how model complexity influences both prediction accuracy and computational performance."><img alt="" src="../_images/sphx_glr_plot_model_complexity_influence_thumb.png" />
<p><a class="reference internal" href="applications/plot_model_complexity_influence.html#sphx-glr-auto-examples-applications-plot-model-complexity-influence-py"><span class="std std-ref">Model Complexity Influence</span></a></p>
  <div class="sphx-glr-thumbnail-title">Model Complexity Influence</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn&#x27;t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful in the case of text classification where new features (words) may appear in each batch."><img alt="" src="../_images/sphx_glr_plot_out_of_core_classification_thumb.png" />
<p><a class="reference internal" href="applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="std std-ref">Out-of-core classification of text documents</span></a></p>
  <div class="sphx-glr-thumbnail-title">Out-of-core classification of text documents</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure."><img alt="" src="../_images/sphx_glr_plot_outlier_detection_wine_thumb.png" />
<p><a class="reference internal" href="applications/plot_outlier_detection_wine.html#sphx-glr-auto-examples-applications-plot-outlier-detection-wine-py"><span class="std std-ref">Outlier detection on a real data set</span></a></p>
  <div class="sphx-glr-thumbnail-title">Outlier detection on a real data set</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This is an example showing the prediction latency of various scikit-learn estimators."><img alt="" src="../_images/sphx_glr_plot_prediction_latency_thumb.png" />
<p><a class="reference internal" href="applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py"><span class="std std-ref">Prediction Latency</span></a></p>
  <div class="sphx-glr-thumbnail-title">Prediction Latency</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Modeling species&#x27; geographic distributions is an important problem in conservation biology. In this example, we model the geographic distribution of two South American mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the OneClassSVM as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses basemap to plot the coast lines and national boundaries of South America."><img alt="" src="../_images/sphx_glr_plot_species_distribution_modeling_thumb.png" />
<p><a class="reference internal" href="applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py"><span class="std std-ref">Species distribution modeling</span></a></p>
  <div class="sphx-glr-thumbnail-title">Species distribution modeling</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This notebook introduces different strategies to leverage time-related features for a bike sharing demand regression task that is highly dependent on business cycles (days, weeks, months) and yearly season cycles."><img alt="" src="../_images/sphx_glr_plot_cyclical_feature_engineering_thumb.png" />
<p><a class="reference internal" href="applications/plot_cyclical_feature_engineering.html#sphx-glr-auto-examples-applications-plot-cyclical-feature-engineering-py"><span class="std std-ref">Time-related feature engineering</span></a></p>
  <div class="sphx-glr-thumbnail-title">Time-related feature engineering</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This is an example of applying NMF and LatentDirichletAllocation on a corpus of documents and extract additive models of the topic structure of the corpus.  The output is a plot of topics, each represented as bar plot using top few words based on weights."><img alt="" src="../_images/sphx_glr_plot_topics_extraction_with_nmf_lda_thumb.png" />
<p><a class="reference internal" href="applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes."><img alt="" src="../_images/sphx_glr_plot_stock_market_thumb.png" />
<p><a class="reference internal" href="applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py"><span class="std std-ref">Visualizing the stock market structure</span></a></p>
  <div class="sphx-glr-thumbnail-title">Visualizing the stock market structure</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A classical way to assert the relative importance of vertices in a graph is to compute the principal eigenvector of the adjacency matrix so as to assign to each vertex the values of the components of the first eigenvector as a centrality score:"><img alt="" src="../_images/sphx_glr_wikipedia_principal_eigenvector_thumb.png" />
<p><a class="reference internal" href="applications/wikipedia_principal_eigenvector.html#sphx-glr-auto-examples-applications-wikipedia-principal-eigenvector-py"><span class="std std-ref">Wikipedia principal eigenvector</span></a></p>
  <div class="sphx-glr-thumbnail-title">Wikipedia principal eigenvector</div>
</div></div></section>
<section id="feature-selection">
<h2>Feature Selection<a class="headerlink" href="#feature-selection" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.feature_selection.html#module-sklearn.feature_selection" title="sklearn.feature_selection"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.feature_selection</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the differences between univariate F-test statistics and mutual information."><img alt="" src="../_images/sphx_glr_plot_f_test_vs_mi_thumb.png" />
<p><a class="reference internal" href="feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py"><span class="std std-ref">Comparison of F-test and mutual information</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of F-test and mutual information</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates and compares two approaches for feature selection: SelectFromModel which is based on feature importance, and SequentialFeatureSelector which relies on a greedy approach."><img alt="" src="../_images/sphx_glr_plot_select_from_model_diabetes_thumb.png" />
<p><a class="reference internal" href="feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py"><span class="std std-ref">Model-based and sequential feature selection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Model-based and sequential feature selection</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how a feature selection can be easily integrated within a machine learning pipeline."><img alt="" src="../_images/sphx_glr_plot_feature_selection_pipeline_thumb.png" />
<p><a class="reference internal" href="feature_selection/plot_feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-pipeline-py"><span class="std std-ref">Pipeline ANOVA SVM</span></a></p>
  <div class="sphx-glr-thumbnail-title">Pipeline ANOVA SVM</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how Recursive Feature Elimination (~sklearn.feature_selection.RFE) can be used to determine the importance of individual pixels for classifying handwritten digits. RFE recursively removes the least significant features, assigning ranks based on their importance, where higher ranking_ values denote lower importance. The ranking is visualized using both shades of blue and pixel annotations for clarity. As expected, pixels positioned at the center of the image tend to be more predictive than those near the edges."><img alt="" src="../_images/sphx_glr_plot_rfe_digits_thumb.png" />
<p><a class="reference internal" href="feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py"><span class="std std-ref">Recursive feature elimination</span></a></p>
  <div class="sphx-glr-thumbnail-title">Recursive feature elimination</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A Recursive Feature Elimination (RFE) example with automatic tuning of the number of features selected with cross-validation."><img alt="" src="../_images/sphx_glr_plot_rfe_with_cross_validation_thumb.png" />
<p><a class="reference internal" href="feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"><span class="std std-ref">Recursive feature elimination with cross-validation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Recursive feature elimination with cross-validation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This notebook is an example of using univariate feature selection to improve classification accuracy on a noisy dataset."><img alt="" src="../_images/sphx_glr_plot_feature_selection_thumb.png" />
<p><a class="reference internal" href="feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py"><span class="std std-ref">Univariate Feature Selection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Univariate Feature Selection</div>
</div></div></section>
<section id="gaussian-mixture-models">
<h2>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.mixture.html#module-sklearn.mixture" title="sklearn.mixture"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.mixture</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the BayesianGaussianMixture class models with a Dirichlet distribution prior (``weight_concentration_prior_type=&#x27;dirichlet_distribution&#x27;``) and a Dirichlet process prior (``weight_concentration_prior_type=&#x27;dirichlet_process&#x27;``). On each figure, we plot the results for three different values of the weight concentration prior."><img alt="" src="../_images/sphx_glr_plot_concentration_prior_thumb.png" />
<p><a class="reference internal" href="mixture/plot_concentration_prior.html#sphx-glr-auto-examples-mixture-plot-concentration-prior-py"><span class="std std-ref">Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture</span></a></p>
  <div class="sphx-glr-thumbnail-title">Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the density estimation of a mixture of two Gaussians. Data is generated from two Gaussians with different centers and covariance matrices."><img alt="" src="../_images/sphx_glr_plot_gmm_pdf_thumb.png" />
<p><a class="reference internal" href="mixture/plot_gmm_pdf.html#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py"><span class="std std-ref">Density Estimation for a Gaussian mixture</span></a></p>
  <div class="sphx-glr-thumbnail-title">Density Estimation for a Gaussian mixture</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Examples of the different methods of initialization in Gaussian Mixture Models"><img alt="" src="../_images/sphx_glr_plot_gmm_init_thumb.png" />
<p><a class="reference internal" href="mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py"><span class="std std-ref">GMM Initialization Methods</span></a></p>
  <div class="sphx-glr-thumbnail-title">GMM Initialization Methods</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Demonstration of several covariances types for Gaussian mixture models."><img alt="" src="../_images/sphx_glr_plot_gmm_covariances_thumb.png" />
<p><a class="reference internal" href="mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py"><span class="std std-ref">GMM covariances</span></a></p>
  <div class="sphx-glr-thumbnail-title">GMM covariances</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the confidence ellipsoids of a mixture of two Gaussians obtained with Expectation Maximisation (``GaussianMixture`` class) and Variational Inference (``BayesianGaussianMixture`` class models with a Dirichlet process prior)."><img alt="" src="../_images/sphx_glr_plot_gmm_thumb.png" />
<p><a class="reference internal" href="mixture/plot_gmm.html#sphx-glr-auto-examples-mixture-plot-gmm-py"><span class="std std-ref">Gaussian Mixture Model Ellipsoids</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gaussian Mixture Model Ellipsoids</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows that model selection can be performed with Gaussian Mixture Models (GMM) using information-theory criteria &lt;aic_bic&gt;. Model selection concerns both the covariance type and the number of components in the model."><img alt="" src="../_images/sphx_glr_plot_gmm_selection_thumb.png" />
<p><a class="reference internal" href="mixture/plot_gmm_selection.html#sphx-glr-auto-examples-mixture-plot-gmm-selection-py"><span class="std std-ref">Gaussian Mixture Model Selection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gaussian Mixture Model Selection</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the behavior of Gaussian mixture models fit on data that was not sampled from a mixture of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There is therefore no ground truth value for the number of Gaussian components."><img alt="" src="../_images/sphx_glr_plot_gmm_sin_thumb.png" />
<p><a class="reference internal" href="mixture/plot_gmm_sin.html#sphx-glr-auto-examples-mixture-plot-gmm-sin-py"><span class="std std-ref">Gaussian Mixture Model Sine Curve</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gaussian Mixture Model Sine Curve</div>
</div></div></section>
<section id="gaussian-process-for-machine-learning">
<h2>Gaussian Process for Machine Learning<a class="headerlink" href="#gaussian-process-for-machine-learning" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.gaussian_process.html#module-sklearn.gaussian_process" title="sklearn.gaussian_process"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.gaussian_process</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example shows the ability of the WhiteKernel to estimate the noise level in the data. Moreover, we show the importance of kernel hyperparameters initialization."><img alt="" src="../_images/sphx_glr_plot_gpr_noisy_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpr_noisy.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-py"><span class="std std-ref">Ability of Gaussian process regression (GPR) to estimate data noise-level</span></a></p>
  <div class="sphx-glr-thumbnail-title">Ability of Gaussian process regression (GPR) to estimate data noise-level</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates differences between a kernel ridge regression and a Gaussian process regression."><img alt="" src="../_images/sphx_glr_plot_compare_gpr_krr_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_compare_gpr_krr.html#sphx-glr-auto-examples-gaussian-process-plot-compare-gpr-krr-py"><span class="std std-ref">Comparison of kernel ridge and Gaussian process regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of kernel ridge and Gaussian process regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example is based on Section 5.4.3 of &quot;Gaussian Processes for Machine Learning&quot; [1]_. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppm)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t and extrapolate for years after 2001."><img alt="" src="../_images/sphx_glr_plot_gpr_co2_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpr_co2.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-co2-py"><span class="std std-ref">Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A simple one-dimensional regression example computed in two different ways:"><img alt="" src="../_images/sphx_glr_plot_gpr_noisy_targets_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py"><span class="std std-ref">Gaussian Processes regression: basic introductory example</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gaussian Processes regression: basic introductory example</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions."><img alt="" src="../_images/sphx_glr_plot_gpc_iris_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpc_iris.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-iris-py"><span class="std std-ref">Gaussian process classification (GPC) on iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gaussian process classification (GPC) on iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the use of Gaussian processes for regression and classification tasks on data that are not in fixed-length feature vector form. This is achieved through the use of kernel functions that operates directly on discrete structures such as variable-length sequences, trees, and graphs."><img alt="" src="../_images/sphx_glr_plot_gpr_on_structured_data_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpr_on_structured_data.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-on-structured-data-py"><span class="std std-ref">Gaussian processes on discrete data structures</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gaussian processes on discrete data structures</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better results."><img alt="" src="../_images/sphx_glr_plot_gpc_xor_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpc_xor.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-xor-py"><span class="std std-ref">Illustration of Gaussian process classification (GPC) on the XOR dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Illustration of Gaussian process classification (GPC) on the XOR dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the prior and posterior of a GaussianProcessRegressor with different kernels. Mean, standard deviation, and 5 samples are shown for both prior and posterior distributions."><img alt="" src="../_images/sphx_glr_plot_gpr_prior_posterior_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py"><span class="std std-ref">Illustration of prior and posterior Gaussian process for different kernels</span></a></p>
  <div class="sphx-glr-thumbnail-title">Illustration of prior and posterior Gaussian process for different kernels</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A two-dimensional classification example showing iso-probability lines for the predicted probabilities."><img alt="" src="../_images/sphx_glr_plot_gpc_isoprobability_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpc_isoprobability.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-isoprobability-py"><span class="std std-ref">Iso-probability lines for Gaussian Processes classification (GPC)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Iso-probability lines for Gaussian Processes classification (GPC)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML)."><img alt="" src="../_images/sphx_glr_plot_gpc_thumb.png" />
<p><a class="reference internal" href="gaussian_process/plot_gpc.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-py"><span class="std std-ref">Probabilistic predictions with Gaussian process classification (GPC)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Probabilistic predictions with Gaussian process classification (GPC)</div>
</div></div></section>
<section id="generalized-linear-models">
<h2>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.linear_model.html#module-sklearn.linear_model" title="sklearn.linear_model"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example compares two different bayesian regressors:"><img alt="" src="../_images/sphx_glr_plot_ard_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ard.html#sphx-glr-auto-examples-linear-model-plot-ard-py"><span class="std std-ref">Comparing Linear Bayesian Regressors</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing Linear Bayesian Regressors</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Comparing various online solvers"><img alt="" src="../_images/sphx_glr_plot_sgd_comparison_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_comparison.html#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py"><span class="std std-ref">Comparing various online solvers</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing various online solvers</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Computes a Bayesian Ridge Regression of Sinusoids."><img alt="" src="../_images/sphx_glr_plot_bayesian_ridge_curvefit_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_bayesian_ridge_curvefit.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-curvefit-py"><span class="std std-ref">Curve Fitting with Bayesian Ridge Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Curve Fitting with Bayesian Ridge Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Stochastic Gradient Descent is an optimization technique which minimizes a loss function in a stochastic fashion, performing a gradient descent step sample by sample. In particular, it is a very efficient method to fit linear models."><img alt="" src="../_images/sphx_glr_plot_sgd_early_stopping_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_early_stopping.html#sphx-glr-auto-examples-linear-model-plot-sgd-early-stopping-py"><span class="std std-ref">Early stopping of Stochastic Gradient Descent</span></a></p>
  <div class="sphx-glr-thumbnail-title">Early stopping of Stochastic Gradient Descent</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The following example shows how to precompute the gram matrix while using weighted samples with an ElasticNet."><img alt="" src="../_images/sphx_glr_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-elastic-net-precomputed-gram-matrix-with-weighted-samples-py"><span class="std std-ref">Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Fit Ridge and HuberRegressor on a dataset with outliers."><img alt="" src="../_images/sphx_glr_plot_huber_vs_ridge_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py"><span class="std std-ref">HuberRegressor vs Ridge on dataset with strong outliers</span></a></p>
  <div class="sphx-glr-thumbnail-title">HuberRegressor vs Ridge on dataset with strong outliers</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable."><img alt="" src="../_images/sphx_glr_plot_multi_task_lasso_support_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py"><span class="std std-ref">Joint feature selection with multi-task Lasso</span></a></p>
  <div class="sphx-glr-thumbnail-title">Joint feature selection with multi-task Lasso</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Comparison of the sparsity (percentage of zero coefficients) of solutions when L1, L2 and Elastic-Net penalty are used for different values of C. We can see that large values of C give more freedom to the model.  Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. As expected, the Elastic-Net penalty sparsity is between that of L1 and L2."><img alt="" src="../_images/sphx_glr_plot_logistic_l1_l2_sparsity_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py"><span class="std std-ref">L1 Penalty and Sparsity in Logistic Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">L1 Penalty and Sparsity in Logistic Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The present example compares three l1-based regression models on a synthetic signal obtained from sparse and correlated features that are further corrupted with additive gaussian noise:"><img alt="" src="../_images/sphx_glr_plot_lasso_and_elasticnet_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="std std-ref">L1-based models for Sparse Signals</span></a></p>
  <div class="sphx-glr-thumbnail-title">L1-based models for Sparse Signals</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent."><img alt="" src="../_images/sphx_glr_plot_lasso_coordinate_descent_path_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py"><span class="std std-ref">Lasso and Elastic Net</span></a></p>
  <div class="sphx-glr-thumbnail-title">Lasso and Elastic Net</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example reproduces the example of Fig. 2 of [ZHT2007]_. A LassoLarsIC estimator is fit on a diabetes dataset and the AIC and the BIC criteria are used to select the best model."><img alt="" src="../_images/sphx_glr_plot_lasso_lars_ic_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_lasso_lars_ic.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-ic-py"><span class="std std-ref">Lasso model selection via information criteria</span></a></p>
  <div class="sphx-glr-thumbnail-title">Lasso model selection via information criteria</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example focuses on model selection for Lasso models that are linear models with an L1 penalty for regression problems."><img alt="" src="../_images/sphx_glr_plot_lasso_model_selection_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py"><span class="std std-ref">Lasso model selection: AIC-BIC / cross-validation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Lasso model selection: AIC-BIC / cross-validation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved."><img alt="" src="../_images/sphx_glr_plot_lasso_dense_vs_sparse_data_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_lasso_dense_vs_sparse_data.html#sphx-glr-auto-examples-linear-model-plot-lasso-dense-vs-sparse-data-py"><span class="std std-ref">Lasso on dense and sparse data</span></a></p>
  <div class="sphx-glr-thumbnail-title">Lasso on dense and sparse data</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter."><img alt="" src="../_images/sphx_glr_plot_lasso_lars_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py"><span class="std std-ref">Lasso path using LARS</span></a></p>
  <div class="sphx-glr-thumbnail-title">Lasso path using LARS</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The coefficients, residual sum of squares and the coefficient of determination are also calculated."><img alt="" src="../_images/sphx_glr_plot_ols_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py"><span class="std std-ref">Linear Regression Example</span></a></p>
  <div class="sphx-glr-thumbnail-title">Linear Regression Example</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Show below is a logistic-regression classifiers decision boundaries on the first two dimensions (sepal length and width) of the iris dataset. The datapoints are colored according to their labels."><img alt="" src="../_images/sphx_glr_plot_iris_logistic_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py"><span class="std std-ref">Logistic Regression 3-class Classifier</span></a></p>
  <div class="sphx-glr-thumbnail-title">Logistic Regression 3-class Classifier</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Shown in the plot is how the logistic regression would, in this synthetic dataset, classify values as either 0 or 1, i.e. class one or two, using the logistic curve."><img alt="" src="../_images/sphx_glr_plot_logistic_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py"><span class="std std-ref">Logistic function</span></a></p>
  <div class="sphx-glr-thumbnail-title">Logistic function</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &gt; 0.8, while weight vectors remains sparse and therefore more easily interpretable."><img alt="" src="../_images/sphx_glr_plot_sparse_logistic_regression_mnist_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py"><span class="std std-ref">MNIST classification using multinomial logistic + L1</span></a></p>
  <div class="sphx-glr-thumbnail-title">MNIST classification using multinomial logistic + L1</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Comparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression to classify documents from the newgroups20 dataset. Multinomial logistic regression yields more accurate results and is faster to train on the larger scale dataset."><img alt="" src="../_images/sphx_glr_plot_sparse_logistic_regression_20newsgroups_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py"><span class="std std-ref">Multiclass sparse logistic regression on 20newgroups</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multiclass sparse logistic regression on 20newgroups</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we fit a linear model with positive constraints on the regression coefficients and compare the estimated coefficients to a classic linear regression."><img alt="" src="../_images/sphx_glr_plot_nnls_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_nnls.html#sphx-glr-auto-examples-linear-model-plot-nnls-py"><span class="std std-ref">Non-negative least squares</span></a></p>
  <div class="sphx-glr-thumbnail-title">Non-negative least squares</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to approximate the solution of sklearn.svm.OneClassSVM in the case of an RBF kernel with sklearn.linear_model.SGDOneClassSVM, a Stochastic Gradient Descent (SGD) version of the One-Class SVM. A kernel approximation is first used in order to apply sklearn.linear_model.SGDOneClassSVM which implements a linear One-Class SVM using SGD."><img alt="" src="../_images/sphx_glr_plot_sgdocsvm_vs_ocsvm_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgdocsvm_vs_ocsvm.html#sphx-glr-auto-examples-linear-model-plot-sgdocsvm-vs-ocsvm-py"><span class="std std-ref">One-Class SVM versus One-Class SVM using Stochastic Gradient Descent</span></a></p>
  <div class="sphx-glr-thumbnail-title">One-Class SVM versus One-Class SVM using Stochastic Gradient Descent</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression"><img alt="" src="../_images/sphx_glr_plot_ols_ridge_variance_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py"><span class="std std-ref">Ordinary Least Squares and Ridge Regression Variance</span></a></p>
  <div class="sphx-glr-thumbnail-title">Ordinary Least Squares and Ridge Regression Variance</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary"><img alt="" src="../_images/sphx_glr_plot_omp_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_omp.html#sphx-glr-auto-examples-linear-model-plot-omp-py"><span class="std std-ref">Orthogonal Matching Pursuit</span></a></p>
  <div class="sphx-glr-thumbnail-title">Orthogonal Matching Pursuit</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Shows the effect of collinearity in the coefficients of an estimator."><img alt="" src="../_images/sphx_glr_plot_ridge_path_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py"><span class="std std-ref">Plot Ridge coefficients as a function of the regularization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot Ridge coefficients as a function of the regularization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines."><img alt="" src="../_images/sphx_glr_plot_sgd_iris_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_iris.html#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py"><span class="std std-ref">Plot multi-class SGD on the iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot multi-class SGD on the iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers are represented by the dashed lines."><img alt="" src="../_images/sphx_glr_plot_logistic_multinomial_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py"><span class="std std-ref">Plot multinomial and One-vs-Rest Logistic Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot multinomial and One-vs-Rest Logistic Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the use of log-linear Poisson regression on the French Motor Third-Party Liability Claims dataset from [1]_ and compares it with a linear model fitted with the usual least squared error and a non-linear GBRT model fitted with the Poisson loss (and a log-link)."><img alt="" src="../_images/sphx_glr_plot_poisson_regression_non_normal_loss_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_poisson_regression_non_normal_loss.html#sphx-glr-auto-examples-linear-model-plot-poisson-regression-non-normal-loss-py"><span class="std std-ref">Poisson regression and non-normal loss</span></a></p>
  <div class="sphx-glr-thumbnail-title">Poisson regression and non-normal loss</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to approximate a function with polynomials up to degree degree by using ridge regression. We show two different ways given n_samples of 1d points x_i:"><img alt="" src="../_images/sphx_glr_plot_polynomial_interpolation_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_polynomial_interpolation.html#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py"><span class="std std-ref">Polynomial and Spline interpolation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Polynomial and Spline interpolation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how quantile regression can predict non-trivial conditional quantiles."><img alt="" src="../_images/sphx_glr_plot_quantile_regression_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_quantile_regression.html#sphx-glr-auto-examples-linear-model-plot-quantile-regression-py"><span class="std std-ref">Quantile regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Quantile regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip=" Train l1-penalized logistic regression models on a binary classification problem derived from the Iris dataset."><img alt="" src="../_images/sphx_glr_plot_logistic_path_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py"><span class="std std-ref">Regularization path of L1- Logistic Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Regularization path of L1- Logistic Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A model that overfits learns the training data too well, capturing both the underlying patterns and the noise in the data. However, when applied to unseen data, the learned associations may not hold. We normally detect this when we apply our trained predictions to the test data and see the statistical performance drop significantly compared to the training data."><img alt="" src="../_images/sphx_glr_plot_ridge_coeffs_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ridge_coeffs.html#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py"><span class="std std-ref">Ridge coefficients as a function of the L2 Regularization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Ridge coefficients as a function of the L2 Regularization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Here a sine function is fit with a polynomial of order 3, for values close to zero."><img alt="" src="../_images/sphx_glr_plot_robust_fit_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="std std-ref">Robust linear estimator fitting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust linear estimator fitting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we see how to robustly fit a linear model to faulty data using the ransac_regression algorithm."><img alt="" src="../_images/sphx_glr_plot_ransac_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py"><span class="std std-ref">Robust linear model estimation using RANSAC</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust linear model estimation using RANSAC</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector Machines classifier trained using SGD."><img alt="" src="../_images/sphx_glr_plot_sgd_separating_hyperplane_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_separating_hyperplane.html#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py"><span class="std std-ref">SGD: Maximum margin separating hyperplane</span></a></p>
  <div class="sphx-glr-thumbnail-title">SGD: Maximum margin separating hyperplane</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Contours of where the penalty is equal to 1 for the three penalties L1, L2 and elastic-net."><img alt="" src="../_images/sphx_glr_plot_sgd_penalties_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_penalties.html#sphx-glr-auto-examples-linear-model-plot-sgd-penalties-py"><span class="std std-ref">SGD: Penalties</span></a></p>
  <div class="sphx-glr-thumbnail-title">SGD: Penalties</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot decision function of a weighted dataset, where the size of points is proportional to its weight."><img alt="" src="../_images/sphx_glr_plot_sgd_weighted_samples_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py"><span class="std std-ref">SGD: Weighted samples</span></a></p>
  <div class="sphx-glr-thumbnail-title">SGD: Weighted samples</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A plot that compares the various convex loss functions supported by SGDClassifier ."><img alt="" src="../_images/sphx_glr_plot_sgd_loss_functions_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_sgd_loss_functions.html#sphx-glr-auto-examples-linear-model-plot-sgd-loss-functions-py"><span class="std std-ref">SGD: convex loss functions</span></a></p>
  <div class="sphx-glr-thumbnail-title">SGD: convex loss functions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding y when compared to just feature 1."><img alt="" src="../_images/sphx_glr_plot_ols_3d_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_ols_3d.html#sphx-glr-auto-examples-linear-model-plot-ols-3d-py"><span class="std std-ref">Sparsity Example: Fitting only features 1  and 2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Sparsity Example: Fitting only features 1  and 2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Computes a Theil-Sen Regression on a synthetic dataset."><img alt="" src="../_images/sphx_glr_plot_theilsen_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="std std-ref">Theil-Sen Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Theil-Sen Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the use of Poisson, Gamma and Tweedie regression on the French Motor Third-Party Liability Claims dataset, and is inspired by an R tutorial [1]_."><img alt="" src="../_images/sphx_glr_plot_tweedie_regression_insurance_claims_thumb.png" />
<p><a class="reference internal" href="linear_model/plot_tweedie_regression_insurance_claims.html#sphx-glr-auto-examples-linear-model-plot-tweedie-regression-insurance-claims-py"><span class="std std-ref">Tweedie regression on insurance claims</span></a></p>
  <div class="sphx-glr-thumbnail-title">Tweedie regression on insurance claims</div>
</div></div></section>
<section id="inspection">
<h2>Inspection<a class="headerlink" href="#inspection" title="Link to this heading">#</a></h2>
<p>Examples related to the <a class="reference internal" href="../api/sklearn.inspection.html#module-sklearn.inspection" title="sklearn.inspection"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.inspection</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In linear models, the target value is modeled as a linear combination of the features (see the linear_model User Guide section for a description of a set of linear models available in scikit-learn). Coefficients in multiple linear models represent the relationship between the given feature, X_i and the target, y, assuming that all the other features remain constant (conditional dependence). This is different from plotting X_i versus y and fitting a linear relationship: in that case all possible values of the other features are taken into account in the estimation (marginal dependence)."><img alt="" src="../_images/sphx_glr_plot_linear_model_coefficient_interpretation_thumb.png" />
<p><a class="reference internal" href="inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py"><span class="std std-ref">Common pitfalls in the interpretation of coefficients of linear models</span></a></p>
  <div class="sphx-glr-thumbnail-title">Common pitfalls in the interpretation of coefficients of linear models</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Machine Learning models are great for measuring statistical associations. Unfortunately, unless we&#x27;re willing to make strong assumptions about the data, those models are unable to infer causal effects."><img alt="" src="../_images/sphx_glr_plot_causal_interpretation_thumb.png" />
<p><a class="reference internal" href="inspection/plot_causal_interpretation.html#sphx-glr-auto-examples-inspection-plot-causal-interpretation-py"><span class="std std-ref">Failure of Machine Learning to infer causal effects</span></a></p>
  <div class="sphx-glr-thumbnail-title">Failure of Machine Learning to infer causal effects</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Partial dependence plots show the dependence between the target function [2]_ and a set of features of interest, marginalizing over the values of all other features (the complement features). Due to the limits of human perception, the size of the set of features of interest must be small (usually, one or two) thus they are usually chosen among the most important features."><img alt="" src="../_images/sphx_glr_plot_partial_dependence_thumb.png" />
<p><a class="reference internal" href="inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py"><span class="std std-ref">Partial Dependence and Individual Conditional Expectation Plots</span></a></p>
  <div class="sphx-glr-thumbnail-title">Partial Dependence and Individual Conditional Expectation Plots</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we will compare the impurity-based feature importance of RandomForestClassifier with the permutation importance on the titanic dataset using permutation_importance. We will show that the impurity-based feature importance can inflate the importance of numerical features."><img alt="" src="../_images/sphx_glr_plot_permutation_importance_thumb.png" />
<p><a class="reference internal" href="inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py"><span class="std std-ref">Permutation Importance vs Random Forest Feature Importance (MDI)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Permutation Importance vs Random Forest Feature Importance (MDI)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we compute the permutation_importance of the features to a trained RandomForestClassifier using the breast_cancer_dataset. The model can easily get about 97% accuracy on a test dataset. Because this dataset contains multicollinear features, the permutation importance shows that none of the features are important, in contradiction with the high test accuracy."><img alt="" src="../_images/sphx_glr_plot_permutation_importance_multicollinear_thumb.png" />
<p><a class="reference internal" href="inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py"><span class="std std-ref">Permutation Importance with Multicollinear or Correlated Features</span></a></p>
  <div class="sphx-glr-thumbnail-title">Permutation Importance with Multicollinear or Correlated Features</div>
</div></div></section>
<section id="kernel-approximation">
<h2>Kernel Approximation<a class="headerlink" href="#kernel-approximation" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.kernel_approximation.html#module-sklearn.kernel_approximation" title="sklearn.kernel_approximation"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.kernel_approximation</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the use of PolynomialCountSketch to efficiently generate polynomial kernel feature-space approximations. This is used to train linear classifiers that approximate the accuracy of kernelized ones."><img alt="" src="../_images/sphx_glr_plot_scalable_poly_kernels_thumb.png" />
<p><a class="reference internal" href="kernel_approximation/plot_scalable_poly_kernels.html#sphx-glr-auto-examples-kernel-approximation-plot-scalable-poly-kernels-py"><span class="std std-ref">Scalable learning with polynomial kernel approximation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Scalable learning with polynomial kernel approximation</div>
</div></div></section>
<section id="manifold-learning">
<h2>Manifold learning<a class="headerlink" href="#manifold-learning" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.manifold.html#module-sklearn.manifold" title="sklearn.manifold"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.manifold</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods."><img alt="" src="../_images/sphx_glr_plot_compare_methods_thumb.png" />
<p><a class="reference internal" href="manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"><span class="std std-ref">Comparison of Manifold Learning methods</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of Manifold Learning methods</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An application of the different manifold techniques on a spherical data-set. Here one can see the use of dimensionality reduction in order to gain some intuition regarding the manifold learning methods. Regarding the dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning techniques to &#x27;spread it open&#x27; whilst projecting it onto two dimensions."><img alt="" src="../_images/sphx_glr_plot_manifold_sphere_thumb.png" />
<p><a class="reference internal" href="manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py"><span class="std std-ref">Manifold Learning methods on a severed sphere</span></a></p>
  <div class="sphx-glr-thumbnail-title">Manifold Learning methods on a severed sphere</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We illustrate various embedding techniques on the digits dataset."><img alt="" src="../_images/sphx_glr_plot_lle_digits_thumb.png" />
<p><a class="reference internal" href="manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</span></a></p>
  <div class="sphx-glr-thumbnail-title">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An illustration of the metric and non-metric MDS on generated noisy data."><img alt="" src="../_images/sphx_glr_plot_mds_thumb.png" />
<p><a class="reference internal" href="manifold/plot_mds.html#sphx-glr-auto-examples-manifold-plot-mds-py"><span class="std std-ref">Multi-dimensional scaling</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multi-dimensional scaling</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Swiss Roll And Swiss-Hole Reduction"><img alt="" src="../_images/sphx_glr_plot_swissroll_thumb.png" />
<p><a class="reference internal" href="manifold/plot_swissroll.html#sphx-glr-auto-examples-manifold-plot-swissroll-py"><span class="std std-ref">Swiss Roll And Swiss-Hole Reduction</span></a></p>
  <div class="sphx-glr-thumbnail-title">Swiss Roll And Swiss-Hole Reduction</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An illustration of t-SNE on the two concentric circles and the S-curve datasets for different perplexity values."><img alt="" src="../_images/sphx_glr_plot_t_sne_perplexity_thumb.png" />
<p><a class="reference internal" href="manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py"><span class="std std-ref">t-SNE: The effect of various perplexity values on the shape</span></a></p>
  <div class="sphx-glr-thumbnail-title">t-SNE: The effect of various perplexity values on the shape</div>
</div></div></section>
<section id="miscellaneous">
<h2>Miscellaneous<a class="headerlink" href="#miscellaneous" title="Link to this heading">#</a></h2>
<p>Miscellaneous and introductory examples for scikit-learn.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="    See also sphx_glr_auto_examples_miscellaneous_plot_roc_curve_visualization_api.py"><img alt="" src="../_images/sphx_glr_plot_partial_dependence_visualization_api_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_partial_dependence_visualization_api.html#sphx-glr-auto-examples-miscellaneous-plot-partial-dependence-visualization-api-py"><span class="std std-ref">Advanced Plotting With Partial Dependence</span></a></p>
  <div class="sphx-glr-thumbnail-title">Advanced Plotting With Partial Dependence</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows characteristics of different anomaly detection algorithms on 2D datasets. Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data."><img alt="" src="../_images/sphx_glr_plot_anomaly_comparison_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_anomaly_comparison.html#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py"><span class="std std-ref">Comparing anomaly detection algorithms for outlier detection on toy datasets</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing anomaly detection algorithms for outlier detection on toy datasets</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Both kernel ridge regression (KRR) and SVR learn a non-linear function by employing the kernel trick, i.e., they learn a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. They differ in the loss functions (ridge versus epsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR at prediction-time."><img alt="" src="../_images/sphx_glr_plot_kernel_ridge_regression_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_kernel_ridge_regression.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-ridge-regression-py"><span class="std std-ref">Comparison of kernel ridge regression and SVR</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison of kernel ridge regression and SVR</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The default configuration for displaying a pipeline in a Jupyter Notebook is &#x27;diagram&#x27; where set_config(display=&#x27;diagram&#x27;). To deactivate HTML representation, use set_config(display=&#x27;text&#x27;)."><img alt="" src="../_images/sphx_glr_plot_pipeline_display_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_pipeline_display.html#sphx-glr-auto-examples-miscellaneous-plot-pipeline-display-py"><span class="std std-ref">Displaying Pipelines</span></a></p>
  <div class="sphx-glr-thumbnail-title">Displaying Pipelines</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates different ways estimators and pipelines can be displayed."><img alt="" src="../_images/sphx_glr_plot_estimator_representation_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_estimator_representation.html#sphx-glr-auto-examples-miscellaneous-plot-estimator-representation-py"><span class="std std-ref">Displaying estimators and complex pipelines</span></a></p>
  <div class="sphx-glr-thumbnail-title">Displaying estimators and complex pipelines</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares two outlier detection algorithms, namely local_outlier_factor (LOF) and isolation_forest (IForest), on real-world datasets available in sklearn.datasets. The goal is to show that different algorithms perform well on different datasets and contrast their training speed and sensitivity to hyperparameters."><img alt="" src="../_images/sphx_glr_plot_outlier_detection_bench_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_outlier_detection_bench.html#sphx-glr-auto-examples-miscellaneous-plot-outlier-detection-bench-py"><span class="std std-ref">Evaluation of outlier detection estimators</span></a></p>
  <div class="sphx-glr-thumbnail-title">Evaluation of outlier detection estimators</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example illustrating the approximation of the feature map of an RBF kernel."><img alt="" src="../_images/sphx_glr_plot_kernel_approximation_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_kernel_approximation.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-approximation-py"><span class="std std-ref">Explicit feature map approximation for RBF kernels</span></a></p>
  <div class="sphx-glr-thumbnail-title">Explicit feature map approximation for RBF kernels</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half."><img alt="" src="../_images/sphx_glr_plot_multioutput_face_completion_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a></p>
  <div class="sphx-glr-thumbnail-title">Face completion with a multi-output estimators</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example will demonstrate the set_output API to configure transformers to output pandas DataFrames. set_output can be configured per estimator by calling the set_output method or globally by setting set_config(transform_output=&quot;pandas&quot;). For details, see SLEP018."><img alt="" src="../_images/sphx_glr_plot_set_output_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py"><span class="std std-ref">Introducing the set_output API</span></a></p>
  <div class="sphx-glr-thumbnail-title">Introducing the set_output API</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An illustration of the isotonic regression on generated data (non-linear monotonic trend with homoscedastic uniform noise)."><img alt="" src="../_images/sphx_glr_plot_isotonic_regression_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_isotonic_regression.html#sphx-glr-auto-examples-miscellaneous-plot-isotonic-regression-py"><span class="std std-ref">Isotonic Regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Isotonic Regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This document shows how you can use the metadata routing mechanism &lt;metadata_routing&gt; in scikit-learn to route metadata to the estimators, scorers, and CV splitters consuming them."><img alt="" src="../_images/sphx_glr_plot_metadata_routing_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_metadata_routing.html#sphx-glr-auto-examples-miscellaneous-plot-metadata-routing-py"><span class="std std-ref">Metadata Routing</span></a></p>
  <div class="sphx-glr-thumbnail-title">Metadata Routing</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:"><img alt="" src="../_images/sphx_glr_plot_multilabel_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_multilabel.html#sphx-glr-auto-examples-miscellaneous-plot-multilabel-py"><span class="std std-ref">Multilabel classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multilabel classification</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="ROC Curve with Visualization API"><img alt="" src="../_images/sphx_glr_plot_roc_curve_visualization_api_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_roc_curve_visualization_api.html#sphx-glr-auto-examples-miscellaneous-plot-roc-curve-visualization-api-py"><span class="std std-ref">ROC Curve with Visualization API</span></a></p>
  <div class="sphx-glr-thumbnail-title">ROC Curve with Visualization API</div>
</div><div class="sphx-glr-thumbcontainer" tooltip=" The `Johnson-Lindenstrauss lemma`_ states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances."><img alt="" src="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-miscellaneous-plot-johnson-lindenstrauss-bound-py"><span class="std std-ref">The Johnson-Lindenstrauss bound for embedding with random projections</span></a></p>
  <div class="sphx-glr-thumbnail-title">The Johnson-Lindenstrauss bound for embedding with random projections</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we will construct display objects, ConfusionMatrixDisplay, RocCurveDisplay, and PrecisionRecallDisplay directly from their respective metrics. This is an alternative to using their corresponding plot functions when a model&#x27;s predictions are already computed or expensive to compute. Note that this is advanced usage, and in general we recommend using their respective plot functions."><img alt="" src="../_images/sphx_glr_plot_display_object_visualization_thumb.png" />
<p><a class="reference internal" href="miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"><span class="std std-ref">Visualizations with Display Objects</span></a></p>
  <div class="sphx-glr-thumbnail-title">Visualizations with Display Objects</div>
</div></div></section>
<section id="missing-value-imputation">
<h2>Missing Value Imputation<a class="headerlink" href="#missing-value-imputation" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.impute.html#module-sklearn.impute" title="sklearn.impute"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.impute</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Missing values can be replaced by the mean, the median or the most frequent value using the basic SimpleImputer."><img alt="" src="../_images/sphx_glr_plot_missing_values_thumb.png" />
<p><a class="reference internal" href="impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py"><span class="std std-ref">Imputing missing values before building an estimator</span></a></p>
  <div class="sphx-glr-thumbnail-title">Imputing missing values before building an estimator</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The IterativeImputer class is very flexible - it can be used with a variety of estimators to do round-robin regression, treating every variable as an output in turn."><img alt="" src="../_images/sphx_glr_plot_iterative_imputer_variants_comparison_thumb.png" />
<p><a class="reference internal" href="impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py"><span class="std std-ref">Imputing missing values with variants of IterativeImputer</span></a></p>
  <div class="sphx-glr-thumbnail-title">Imputing missing values with variants of IterativeImputer</div>
</div></div></section>
<section id="model-selection">
<h2>Model Selection<a class="headerlink" href="#model-selection" title="Link to this heading">#</a></h2>
<p>Examples related to the <a class="reference internal" href="../api/sklearn.model_selection.html#module-sklearn.model_selection" title="sklearn.model_selection"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example balances model complexity and cross-validated score by finding a decent accuracy within 1 standard deviation of the best accuracy score while minimising the number of PCA components [1]."><img alt="" src="../_images/sphx_glr_plot_grid_search_refit_callable_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_grid_search_refit_callable.html#sphx-glr-auto-examples-model-selection-plot-grid-search-refit-callable-py"><span class="std std-ref">Balance model complexity and cross-validated score</span></a></p>
  <div class="sphx-glr-thumbnail-title">Balance model complexity and cross-validated score</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the class_likelihood_ratios function, which computes the positive and negative likelihood ratios (`LR+`, LR-) to assess the predictive power of a binary classifier. As we will see, these metrics are independent of the proportion between classes in the test set, which makes them very useful when the available data for a study has a different class proportion than the target application."><img alt="" src="../_images/sphx_glr_plot_likelihood_ratios_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_likelihood_ratios.html#sphx-glr-auto-examples-model-selection-plot-likelihood-ratios-py"><span class="std std-ref">Class Likelihood Ratios to measure classification performance</span></a></p>
  <div class="sphx-glr-thumbnail-title">Class Likelihood Ratios to measure classification performance</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Compare randomized search and grid search for optimizing hyperparameters of a linear SVM with SGD training. All parameters that influence the learning are searched simultaneously (except for the number of estimators, which poses a time / quality tradeoff)."><img alt="" src="../_images/sphx_glr_plot_randomized_search_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py"><span class="std std-ref">Comparing randomized search and grid search for hyperparameter estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing randomized search and grid search for hyperparameter estimation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares the parameter search performed by HalvingGridSearchCV and GridSearchCV."><img alt="" src="../_images/sphx_glr_plot_successive_halving_heatmap_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_successive_halving_heatmap.html#sphx-glr-auto-examples-model-selection-plot-successive-halving-heatmap-py"><span class="std std-ref">Comparison between grid search and successive halving</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparison between grid search and successive halving</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions."><img alt="" src="../_images/sphx_glr_plot_confusion_matrix_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"><span class="std std-ref">Confusion matrix</span></a></p>
  <div class="sphx-glr-thumbnail-title">Confusion matrix</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This examples shows how a classifier is optimized by cross-validation, which is done using the GridSearchCV object on a development set that comprises only half of the available labeled data."><img alt="" src="../_images/sphx_glr_plot_grid_search_digits_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"><span class="std std-ref">Custom refit strategy of a grid search with cross-validation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Custom refit strategy of a grid search with cross-validation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Multiple metric parameter search can be done by setting the scoring parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables."><img alt="" src="../_images/sphx_glr_plot_multi_metric_evaluation_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py"><span class="std std-ref">Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we compare two binary classification multi-threshold metrics: the Receiver Operating Characteristic (ROC) and the Detection Error Tradeoff (DET). For such purpose, we evaluate two different classifiers for the same classification task."><img alt="" src="../_images/sphx_glr_plot_det_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_det.html#sphx-glr-auto-examples-model-selection-plot-det-py"><span class="std std-ref">Detection error tradeoff (DET) curve</span></a></p>
  <div class="sphx-glr-thumbnail-title">Detection error tradeoff (DET) curve</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example describes the use of the Receiver Operating Characteristic (ROC) metric to evaluate the quality of multiclass classifiers."><img alt="" src="../_images/sphx_glr_plot_roc_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py"><span class="std std-ref">Multiclass Receiver Operating Characteristic (ROC)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multiclass Receiver Operating Characteristic (ROC)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score."><img alt="" src="../_images/sphx_glr_plot_nested_cross_validation_iris_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py"><span class="std std-ref">Nested versus non-nested cross-validation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Nested versus non-nested cross-validation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to use cross_val_predict together with PredictionErrorDisplay to visualize prediction errors."><img alt="" src="../_images/sphx_glr_plot_cv_predict_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py"><span class="std std-ref">Plotting Cross-Validated Predictions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plotting Cross-Validated Predictions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we show how to use the class LearningCurveDisplay to easily plot learning curves. In addition, we give an interpretation to the learning curves obtained for a naive Bayes and SVM classifiers."><img alt="" src="../_images/sphx_glr_plot_learning_curve_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py"><span class="std std-ref">Plotting Learning Curves and Checking Models’ Scalability</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plotting Learning Curves and Checking Models' Scalability</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma. For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well. If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor."><img alt="" src="../_images/sphx_glr_plot_validation_curve_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py"><span class="std std-ref">Plotting Validation Curves</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plotting Validation Curves</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Once a binary classifier is trained, the predict method outputs class label predictions corresponding to a thresholding of either the decision_function or the predict_proba output. The default threshold is defined as a posterior probability estimate of 0.5 or a decision score of 0.0. However, this default strategy may not be optimal for the task at hand."><img alt="" src="../_images/sphx_glr_plot_tuned_decision_threshold_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_tuned_decision_threshold.html#sphx-glr-auto-examples-model-selection-plot-tuned-decision-threshold-py"><span class="std std-ref">Post-hoc tuning the cut-off point of decision function</span></a></p>
  <div class="sphx-glr-thumbnail-title">Post-hoc tuning the cut-off point of decision function</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Once a classifier is trained, the output of the predict method outputs class label predictions corresponding to a thresholding of either the decision_function or the predict_proba output. For a binary classifier, the default threshold is defined as a posterior probability estimate of 0.5 or a decision score of 0.0."><img alt="" src="../_images/sphx_glr_plot_cost_sensitive_learning_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_cost_sensitive_learning.html#sphx-glr-auto-examples-model-selection-plot-cost-sensitive-learning-py"><span class="std std-ref">Post-tuning the decision threshold for cost-sensitive learning</span></a></p>
  <div class="sphx-glr-thumbnail-title">Post-tuning the decision threshold for cost-sensitive learning</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Example of Precision-Recall metric to evaluate classifier output quality."><img alt="" src="../_images/sphx_glr_plot_precision_recall_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"><span class="std std-ref">Precision-Recall</span></a></p>
  <div class="sphx-glr-thumbnail-title">Precision-Recall</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example presents how to estimate and visualize the variance of the Receiver Operating Characteristic (ROC) metric using cross-validation."><img alt="" src="../_images/sphx_glr_plot_roc_crossval_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py"><span class="std std-ref">Receiver Operating Characteristic (ROC) with cross validation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Receiver Operating Characteristic (ROC) with cross validation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The dataset used in this example is 20newsgroups_dataset which will be automatically downloaded, cached and reused for the document classification example."><img alt="" src="../_images/sphx_glr_plot_grid_search_text_feature_extraction_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-plot-grid-search-text-feature-extraction-py"><span class="std std-ref">Sample pipeline for text feature extraction and evaluation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Sample pipeline for text feature extraction and evaluation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to statistically compare the performance of models trained and evaluated using GridSearchCV."><img alt="" src="../_images/sphx_glr_plot_grid_search_stats_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_grid_search_stats.html#sphx-glr-auto-examples-model-selection-plot-grid-search-stats-py"><span class="std std-ref">Statistical comparison of models using grid search</span></a></p>
  <div class="sphx-glr-thumbnail-title">Statistical comparison of models using grid search</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how a successive halving search (~sklearn.model_selection.HalvingGridSearchCV and HalvingRandomSearchCV) iteratively chooses the best parameter combination out of multiple candidates."><img alt="" src="../_images/sphx_glr_plot_successive_halving_iterations_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_successive_halving_iterations.html#sphx-glr-auto-examples-model-selection-plot-successive-halving-iterations-py"><span class="std std-ref">Successive Halving Iterations</span></a></p>
  <div class="sphx-glr-thumbnail-title">Successive Halving Iterations</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the use of permutation_test_score to evaluate the significance of a cross-validated score using permutations."><img alt="" src="../_images/sphx_glr_plot_permutation_tests_for_classification_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_permutation_tests_for_classification.html#sphx-glr-auto-examples-model-selection-plot-permutation-tests-for-classification-py"><span class="std std-ref">Test with permutations the significance of a classification score</span></a></p>
  <div class="sphx-glr-thumbnail-title">Test with permutations the significance of a classification score</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2."><img alt="" src="../_images/sphx_glr_plot_train_error_vs_test_error_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_train_error_vs_test_error.html#sphx-glr-auto-examples-model-selection-plot-train-error-vs-test-error-py"><span class="std std-ref">Train error vs Test error</span></a></p>
  <div class="sphx-glr-thumbnail-title">Train error vs Test error</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data."><img alt="" src="../_images/sphx_glr_plot_underfitting_overfitting_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py"><span class="std std-ref">Underfitting vs. Overfitting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Underfitting vs. Overfitting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Choosing the right cross-validation object is a crucial part of fitting a model properly. There are many ways to split data into training and test sets in order to avoid model overfitting, to standardize the number of groups in test sets, etc."><img alt="" src="../_images/sphx_glr_plot_cv_indices_thumb.png" />
<p><a class="reference internal" href="model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"><span class="std std-ref">Visualizing cross-validation behavior in scikit-learn</span></a></p>
  <div class="sphx-glr-thumbnail-title">Visualizing cross-validation behavior in scikit-learn</div>
</div></div></section>
<section id="multiclass-methods">
<h2>Multiclass methods<a class="headerlink" href="#multiclass-methods" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.multiclass.html#module-sklearn.multiclass" title="sklearn.multiclass"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.multiclass</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this example, we discuss the problem of classification when the target variable is composed of more than two classes. This is called multiclass classification."><img alt="" src="../_images/sphx_glr_plot_multiclass_overview_thumb.png" />
<p><a class="reference internal" href="multiclass/plot_multiclass_overview.html#sphx-glr-auto-examples-multiclass-plot-multiclass-overview-py"><span class="std std-ref">Overview of multiclass training meta-estimators</span></a></p>
  <div class="sphx-glr-thumbnail-title">Overview of multiclass training meta-estimators</div>
</div></div></section>
<section id="multioutput-methods">
<h2>Multioutput methods<a class="headerlink" href="#multioutput-methods" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.multioutput.html#module-sklearn.multioutput" title="sklearn.multioutput"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.multioutput</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The most naive strategy to solve such a task is to independently train a binary classifier on each label (i.e. each column of the target variable). At prediction time, the ensemble of binary classifiers is used to assemble multitask prediction."><img alt="" src="../_images/sphx_glr_plot_classifier_chain_yeast_thumb.png" />
<p><a class="reference internal" href="multioutput/plot_classifier_chain_yeast.html#sphx-glr-auto-examples-multioutput-plot-classifier-chain-yeast-py"><span class="std std-ref">Multilabel classification using a classifier chain</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multilabel classification using a classifier chain</div>
</div></div></section>
<section id="nearest-neighbors">
<h2>Nearest Neighbors<a class="headerlink" href="#nearest-neighbors" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.neighbors.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example presents how to chain KNeighborsTransformer and TSNE in a pipeline. It also shows how to wrap the packages nmslib and pynndescent to replace KNeighborsTransformer and perform approximate nearest neighbors. These packages can be installed with pip install nmslib pynndescent."><img alt="" src="../_images/sphx_glr_approximate_nearest_neighbors_thumb.png" />
<p><a class="reference internal" href="neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py"><span class="std std-ref">Approximate nearest neighbors in TSNE</span></a></p>
  <div class="sphx-glr-thumbnail-title">Approximate nearest neighbors in TSNE</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This examples demonstrates how to precompute the k nearest neighbors before using them in KNeighborsClassifier. KNeighborsClassifier can compute the nearest neighbors internally, but precomputing them can have several benefits, such as finer parameter control, caching for multiple use, or custom implementations."><img alt="" src="../_images/sphx_glr_plot_caching_nearest_neighbors_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_caching_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-plot-caching-nearest-neighbors-py"><span class="std std-ref">Caching nearest neighbors</span></a></p>
  <div class="sphx-glr-thumbnail-title">Caching nearest neighbors</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example comparing nearest neighbors classification with and without Neighborhood Components Analysis."><img alt="" src="../_images/sphx_glr_plot_nca_classification_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_nca_classification.html#sphx-glr-auto-examples-neighbors-plot-nca-classification-py"><span class="std std-ref">Comparing Nearest Neighbors with and without Neighborhood Components Analysis</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing Nearest Neighbors with and without Neighborhood Components Analysis</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Sample usage of Neighborhood Components Analysis for dimensionality reduction."><img alt="" src="../_images/sphx_glr_plot_nca_dim_reduction_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_nca_dim_reduction.html#sphx-glr-auto-examples-neighbors-plot-nca-dim-reduction-py"><span class="std std-ref">Dimensionality Reduction with Neighborhood Components Analysis</span></a></p>
  <div class="sphx-glr-thumbnail-title">Dimensionality Reduction with Neighborhood Components Analysis</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example does not perform any learning over the data (see sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py for an example of classification based on the attributes in this dataset).  It simply shows the kernel density estimate of observed data points in geospatial coordinates."><img alt="" src="../_images/sphx_glr_plot_species_kde_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py"><span class="std std-ref">Kernel Density Estimate of Species Distributions</span></a></p>
  <div class="sphx-glr-thumbnail-title">Kernel Density Estimate of Species Distributions</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset.  With this generative model in place, new samples can be drawn.  These new samples reflect the underlying model of the data."><img alt="" src="../_images/sphx_glr_plot_digits_kde_sampling_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_digits_kde_sampling.html#sphx-glr-auto-examples-neighbors-plot-digits-kde-sampling-py"><span class="std std-ref">Kernel Density Estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Kernel Density Estimation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Sample usage of Nearest Centroid classification. It will plot the decision boundaries for each class."><img alt="" src="../_images/sphx_glr_plot_nearest_centroid_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py"><span class="std std-ref">Nearest Centroid Classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Nearest Centroid Classification</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to use KNeighborsClassifier. We train such a classifier on the iris dataset and observe the difference of the decision boundary obtained with regards to the parameter weights."><img alt="" src="../_images/sphx_glr_plot_classification_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="std std-ref">Nearest Neighbors Classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Nearest Neighbors Classification</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Demonstrate the resolution of a regression problem using a k-Nearest Neighbor and the interpolation of the target using both barycenter and constant weights."><img alt="" src="../_images/sphx_glr_plot_regression_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py"><span class="std std-ref">Nearest Neighbors regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Nearest Neighbors regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates a learned distance metric that maximizes the nearest neighbors classification accuracy. It provides a visual representation of this metric compared to the original point space. Please refer to the User Guide &lt;nca&gt; for more information."><img alt="" src="../_images/sphx_glr_plot_nca_illustration_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_nca_illustration.html#sphx-glr-auto-examples-neighbors-plot-nca-illustration-py"><span class="std std-ref">Neighborhood Components Analysis Illustration</span></a></p>
  <div class="sphx-glr-thumbnail-title">Neighborhood Components Analysis Illustration</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See User Guide &lt;outlier_detection&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection."><img alt="" src="../_images/sphx_glr_plot_lof_novelty_detection_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_lof_novelty_detection.html#sphx-glr-auto-examples-neighbors-plot-lof-novelty-detection-py"><span class="std std-ref">Novelty detection with Local Outlier Factor (LOF)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Novelty detection with Local Outlier Factor (LOF)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See the User Guide &lt;outlier_detection&gt; for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection."><img alt="" src="../_images/sphx_glr_plot_lof_outlier_detection_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_lof_outlier_detection.html#sphx-glr-auto-examples-neighbors-plot-lof-outlier-detection-py"><span class="std std-ref">Outlier detection with Local Outlier Factor (LOF)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Outlier detection with Local Outlier Factor (LOF)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &quot;block&quot; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution.  If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel.  This is a kernel density estimation with a &quot;top hat&quot; kernel.  This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution."><img alt="" src="../_images/sphx_glr_plot_kde_1d_thumb.png" />
<p><a class="reference internal" href="neighbors/plot_kde_1d.html#sphx-glr-auto-examples-neighbors-plot-kde-1d-py"><span class="std std-ref">Simple 1D Kernel Density Estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Simple 1D Kernel Density Estimation</div>
</div></div></section>
<section id="neural-networks">
<h2>Neural Networks<a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.neural_network.html#module-sklearn.neural_network" title="sklearn.neural_network"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neural_network</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however."><img alt="" src="../_images/sphx_glr_plot_mlp_training_curves_thumb.png" />
<p><a class="reference internal" href="neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py"><span class="std std-ref">Compare Stochastic learning strategies for MLPClassifier</span></a></p>
  <div class="sphx-glr-thumbnail-title">Compare Stochastic learning strategies for MLPClassifier</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (BernoulliRBM) can perform effective non-linear feature extraction."><img alt="" src="../_images/sphx_glr_plot_rbm_logistic_classification_thumb.png" />
<p><a class="reference internal" href="neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py"><span class="std std-ref">Restricted Boltzmann Machine features for digit classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Restricted Boltzmann Machine features for digit classification</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A comparison of different values for regularization parameter &#x27;alpha&#x27; on synthetic datasets. The plot shows that different alphas yield different decision functions."><img alt="" src="../_images/sphx_glr_plot_mlp_alpha_thumb.png" />
<p><a class="reference internal" href="neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py"><span class="std std-ref">Varying regularization in Multi-layer Perceptron</span></a></p>
  <div class="sphx-glr-thumbnail-title">Varying regularization in Multi-layer Perceptron</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Sometimes looking at the learned coefficients of a neural network can provide insight into the learning behavior. For example if weights look unstructured, maybe some were not used at all, or if very large coefficients exist, maybe regularization was too low or the learning rate too high."><img alt="" src="../_images/sphx_glr_plot_mnist_filters_thumb.png" />
<p><a class="reference internal" href="neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py"><span class="std std-ref">Visualization of MLP weights on MNIST</span></a></p>
  <div class="sphx-glr-thumbnail-title">Visualization of MLP weights on MNIST</div>
</div></div></section>
<section id="pipelines-and-composite-estimators">
<h2>Pipelines and composite estimators<a class="headerlink" href="#pipelines-and-composite-estimators" title="Link to this heading">#</a></h2>
<p>Examples of how to compose transformers and pipelines from other estimators. See the <a class="reference internal" href="../modules/compose.html#combining-estimators"><span class="std std-ref">User Guide</span></a>.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Datasets can often contain components that require different feature extraction and processing pipelines. This scenario might occur when:"><img alt="" src="../_images/sphx_glr_plot_column_transformer_thumb.png" />
<p><a class="reference internal" href="compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py"><span class="std std-ref">Column Transformer with Heterogeneous Data Sources</span></a></p>
  <div class="sphx-glr-thumbnail-title">Column Transformer with Heterogeneous Data Sources</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how to apply different preprocessing and feature extraction pipelines to different subsets of features, using ColumnTransformer. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones."><img alt="" src="../_images/sphx_glr_plot_column_transformer_mixed_types_thumb.png" />
<p><a class="reference internal" href="compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py"><span class="std std-ref">Column Transformer with Mixed Types</span></a></p>
  <div class="sphx-glr-thumbnail-title">Column Transformer with Mixed Types</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection."><img alt="" src="../_images/sphx_glr_plot_feature_union_thumb.png" />
<p><a class="reference internal" href="compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py"><span class="std std-ref">Concatenating multiple feature extraction methods</span></a></p>
  <div class="sphx-glr-thumbnail-title">Concatenating multiple feature extraction methods</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we give an overview of TransformedTargetRegressor. We use two examples to illustrate the benefit of transforming the targets before learning a linear regression model. The first example uses synthetic data while the second example is based on the Ames housing data set."><img alt="" src="../_images/sphx_glr_plot_transformed_target_thumb.png" />
<p><a class="reference internal" href="compose/plot_transformed_target.html#sphx-glr-auto-examples-compose-plot-transformed-target-py"><span class="std std-ref">Effect of transforming the targets in regression model</span></a></p>
  <div class="sphx-glr-thumbnail-title">Effect of transforming the targets in regression model</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction."><img alt="" src="../_images/sphx_glr_plot_digits_pipe_thumb.png" />
<p><a class="reference internal" href="compose/plot_digits_pipe.html#sphx-glr-auto-examples-compose-plot-digits-pipe-py"><span class="std std-ref">Pipelining: chaining a PCA and a logistic regression</span></a></p>
  <div class="sphx-glr-thumbnail-title">Pipelining: chaining a PCA and a logistic regression</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector classifier. It demonstrates the use of GridSearchCV and Pipeline to optimize over different classes of estimators in a single CV run -- unsupervised PCA and NMF dimensionality reductions are compared to univariate feature selection during the grid search."><img alt="" src="../_images/sphx_glr_plot_compare_reduction_thumb.png" />
<p><a class="reference internal" href="compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py"><span class="std std-ref">Selecting dimensionality reduction with Pipeline and GridSearchCV</span></a></p>
  <div class="sphx-glr-thumbnail-title">Selecting dimensionality reduction with Pipeline and GridSearchCV</div>
</div></div></section>
<section id="preprocessing">
<h2>Preprocessing<a class="headerlink" href="#preprocessing" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.preprocessing.html#module-sklearn.preprocessing" title="sklearn.preprocessing"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.preprocessing</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Feature 0 (median income in a block) and feature 5 (average house occupancy) of the california_housing_dataset have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators."><img alt="" src="../_images/sphx_glr_plot_all_scaling_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"><span class="std std-ref">Compare the effect of different scalers on data with outliers</span></a></p>
  <div class="sphx-glr-thumbnail-title">Compare the effect of different scalers on data with outliers</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The TargetEncoder uses the value of the target to encode each categorical feature. In this example, we will compare three different approaches for handling categorical features: TargetEncoder, OrdinalEncoder, OneHotEncoder and dropping the category."><img alt="" src="../_images/sphx_glr_plot_target_encoder_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_target_encoder.html#sphx-glr-auto-examples-preprocessing-plot-target-encoder-py"><span class="std std-ref">Comparing Target Encoder with Other Encoders</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing Target Encoder with Other Encoders</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example presents the different strategies implemented in KBinsDiscretizer:"><img alt="" src="../_images/sphx_glr_plot_discretization_strategies_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_discretization_strategies.html#sphx-glr-auto-examples-preprocessing-plot-discretization-strategies-py"><span class="std std-ref">Demonstrating the different strategies of KBinsDiscretizer</span></a></p>
  <div class="sphx-glr-thumbnail-title">Demonstrating the different strategies of KBinsDiscretizer</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A demonstration of feature discretization on synthetic classification datasets. Feature discretization decomposes each feature into a set of bins, here equally distributed in width. The discrete values are then one-hot encoded, and given to a linear classifier. This preprocessing enables a non-linear behavior even though the classifier is linear."><img alt="" src="../_images/sphx_glr_plot_discretization_classification_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_discretization_classification.html#sphx-glr-auto-examples-preprocessing-plot-discretization-classification-py"><span class="std std-ref">Feature discretization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Feature discretization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Feature scaling through standardization, also called Z-score normalization, is an important preprocessing step for many machine learning algorithms. It involves rescaling each feature such that it has a standard deviation of 1 and a mean of 0."><img alt="" src="../_images/sphx_glr_plot_scaling_importance_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py"><span class="std std-ref">Importance of Feature Scaling</span></a></p>
  <div class="sphx-glr-thumbnail-title">Importance of Feature Scaling</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through PowerTransformer to map data from various distributions to a normal distribution."><img alt="" src="../_images/sphx_glr_plot_map_data_to_normal_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_map_data_to_normal.html#sphx-glr-auto-examples-preprocessing-plot-map-data-to-normal-py"><span class="std std-ref">Map data to a normal distribution</span></a></p>
  <div class="sphx-glr-thumbnail-title">Map data to a normal distribution</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The TargetEncoder replaces each category of a categorical feature with the shrunk mean of the target variable for that category. This method is useful in cases where there is a strong relationship between the categorical feature and the target. To prevent overfitting, TargetEncoder.fit_transform uses an internal cross fitting scheme to encode the training data to be used by a downstream model. This scheme involves splitting the data into k folds and encoding each fold using the encodings learnt using the other k-1 folds. In this example, we demonstrate the importance of the cross fitting procedure to prevent overfitting."><img alt="" src="../_images/sphx_glr_plot_target_encoder_cross_val_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_target_encoder_cross_val.html#sphx-glr-auto-examples-preprocessing-plot-target-encoder-cross-val-py"><span class="std std-ref">Target Encoder’s Internal Cross fitting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Target Encoder's Internal Cross fitting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features."><img alt="" src="../_images/sphx_glr_plot_discretization_thumb.png" />
<p><a class="reference internal" href="preprocessing/plot_discretization.html#sphx-glr-auto-examples-preprocessing-plot-discretization-py"><span class="std std-ref">Using KBinsDiscretizer to discretize continuous features</span></a></p>
  <div class="sphx-glr-thumbnail-title">Using KBinsDiscretizer to discretize continuous features</div>
</div></div></section>
<section id="semi-supervised-classification">
<h2>Semi Supervised Classification<a class="headerlink" href="#semi-supervised-classification" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.semi_supervised.html#module-sklearn.semi_supervised" title="sklearn.semi_supervised"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.semi_supervised</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A comparison for the decision boundaries generated on the iris dataset by Label Spreading, Self-training and SVM."><img alt="" src="../_images/sphx_glr_plot_semi_supervised_versus_svm_iris_thumb.png" />
<p><a class="reference internal" href="semi_supervised/plot_semi_supervised_versus_svm_iris.html#sphx-glr-auto-examples-semi-supervised-plot-semi-supervised-versus-svm-iris-py"><span class="std std-ref">Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the effect of a varying threshold on self-training. The breast_cancer dataset is loaded, and labels are deleted such that only 50 out of 569 samples have labels. A SelfTrainingClassifier is fitted on this dataset, with varying thresholds."><img alt="" src="../_images/sphx_glr_plot_self_training_varying_threshold_thumb.png" />
<p><a class="reference internal" href="semi_supervised/plot_self_training_varying_threshold.html#sphx-glr-auto-examples-semi-supervised-plot-self-training-varying-threshold-py"><span class="std std-ref">Effect of varying threshold for self-training</span></a></p>
  <div class="sphx-glr-thumbnail-title">Effect of varying threshold for self-training</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Demonstrates an active learning technique to learn handwritten digits using label propagation."><img alt="" src="../_images/sphx_glr_plot_label_propagation_digits_active_learning_thumb.png" />
<p><a class="reference internal" href="semi_supervised/plot_label_propagation_digits_active_learning.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-active-learning-py"><span class="std std-ref">Label Propagation digits active learning</span></a></p>
  <div class="sphx-glr-thumbnail-title">Label Propagation digits active learning</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels."><img alt="" src="../_images/sphx_glr_plot_label_propagation_digits_thumb.png" />
<p><a class="reference internal" href="semi_supervised/plot_label_propagation_digits.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-py"><span class="std std-ref">Label Propagation digits: Demonstrating performance</span></a></p>
  <div class="sphx-glr-thumbnail-title">Label Propagation digits: Demonstrating performance</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Example of LabelPropagation learning a complex internal structure to demonstrate &quot;manifold learning&quot;. The outer circle should be labeled &quot;red&quot; and the inner circle &quot;blue&quot;. Because both label groups lie inside their own distinct shape, we can see that the labels propagate correctly around the circle."><img alt="" src="../_images/sphx_glr_plot_label_propagation_structure_thumb.png" />
<p><a class="reference internal" href="semi_supervised/plot_label_propagation_structure.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-structure-py"><span class="std std-ref">Label Propagation learning a complex structure</span></a></p>
  <div class="sphx-glr-thumbnail-title">Label Propagation learning a complex structure</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, semi-supervised classifiers are trained on the 20 newsgroups dataset (which will be automatically downloaded)."><img alt="" src="../_images/sphx_glr_plot_semi_supervised_newsgroups_thumb.png" />
<p><a class="reference internal" href="semi_supervised/plot_semi_supervised_newsgroups.html#sphx-glr-auto-examples-semi-supervised-plot-semi-supervised-newsgroups-py"><span class="std std-ref">Semi-supervised Classification on a Text Dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Semi-supervised Classification on a Text Dataset</div>
</div></div></section>
<section id="support-vector-machines">
<h2>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.svm.html#module-sklearn.svm" title="sklearn.svm"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.svm</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="An example using a one-class SVM for novelty detection."><img alt="" src="../_images/sphx_glr_plot_oneclass_thumb.png" />
<p><a class="reference internal" href="svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py"><span class="std std-ref">One-class SVM with non-linear kernel (RBF)</span></a></p>
  <div class="sphx-glr-thumbnail-title">One-class SVM with non-linear kernel (RBF)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="SVCs aim to find a hyperplane that effectively separates the classes in their training data by maximizing the margin between the outermost data points of each class. This is achieved by finding the best weight vector w that defines the decision boundary hyperplane and minimizes the sum of hinge losses for misclassified samples, as measured by the hinge_loss function. By default, regularization is applied with the parameter C=1, which allows for a certain degree of misclassification tolerance."><img alt="" src="../_images/sphx_glr_plot_svm_kernels_thumb.png" />
<p><a class="reference internal" href="svm/plot_svm_kernels.html#sphx-glr-auto-examples-svm-plot-svm-kernels-py"><span class="std std-ref">Plot classification boundaries with different SVM Kernels</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot classification boundaries with different SVM Kernels</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset:"><img alt="" src="../_images/sphx_glr_plot_iris_svc_thumb.png" />
<p><a class="reference internal" href="svm/plot_iris_svc.html#sphx-glr-auto-examples-svm-plot-iris-svc-py"><span class="std std-ref">Plot different SVM classifiers in the iris dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot different SVM classifiers in the iris dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Unlike SVC (based on LIBSVM), LinearSVC (based on LIBLINEAR) does not provide the support vectors. This example demonstrates how to obtain the support vectors in LinearSVC."><img alt="" src="../_images/sphx_glr_plot_linearsvc_support_vectors_thumb.png" />
<p><a class="reference internal" href="svm/plot_linearsvc_support_vectors.html#sphx-glr-auto-examples-svm-plot-linearsvc-support-vectors-py"><span class="std std-ref">Plot the support vectors in LinearSVC</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plot the support vectors in LinearSVC</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the effect of the parameters gamma and C of the Radial Basis Function (RBF) kernel SVM."><img alt="" src="../_images/sphx_glr_plot_rbf_parameters_thumb.png" />
<p><a class="reference internal" href="svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py"><span class="std std-ref">RBF SVM parameters</span></a></p>
  <div class="sphx-glr-thumbnail-title">RBF SVM parameters</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A small value of C includes more/all the observations, allowing the margins to be calculated using all the data in the area."><img alt="" src="../_images/sphx_glr_plot_svm_margin_thumb.png" />
<p><a class="reference internal" href="svm/plot_svm_margin.html#sphx-glr-auto-examples-svm-plot-svm-margin-py"><span class="std std-ref">SVM Margins Example</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM Margins Example</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The two plots differ only in the area in the middle where the classes are tied. If break_ties=False, all input in that area would be classified as one class, whereas if break_ties=True, the tie-breaking mechanism will create a non-convex decision boundary in that area."><img alt="" src="../_images/sphx_glr_plot_svm_tie_breaking_thumb.png" />
<p><a class="reference internal" href="svm/plot_svm_tie_breaking.html#sphx-glr-auto-examples-svm-plot-svm-tie-breaking-py"><span class="std std-ref">SVM Tie Breaking Example</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM Tie Breaking Example</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors."><img alt="" src="../_images/sphx_glr_plot_custom_kernel_thumb.png" />
<p><a class="reference internal" href="svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py"><span class="std std-ref">SVM with custom kernel</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM with custom kernel</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores. We use the iris dataset (4 features) and add 36 non-informative features. We can find that our model achieves best performance when we select around 10% of features."><img alt="" src="../_images/sphx_glr_plot_svm_anova_thumb.png" />
<p><a class="reference internal" href="svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py"><span class="std std-ref">SVM-Anova: SVM with univariate feature selection</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM-Anova: SVM with univariate feature selection</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine classifier with linear kernel."><img alt="" src="../_images/sphx_glr_plot_separating_hyperplane_thumb.png" />
<p><a class="reference internal" href="svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py"><span class="std std-ref">SVM: Maximum margin separating hyperplane</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM: Maximum margin separating hyperplane</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Find the optimal separating hyperplane using an SVC for classes that are unbalanced."><img alt="" src="../_images/sphx_glr_plot_separating_hyperplane_unbalanced_thumb.png" />
<p><a class="reference internal" href="svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><span class="std std-ref">SVM: Separating hyperplane for unbalanced classes</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM: Separating hyperplane for unbalanced classes</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plot decision function of a weighted dataset, where the size of points is proportional to its weight."><img alt="" src="../_images/sphx_glr_plot_weighted_samples_thumb.png" />
<p><a class="reference internal" href="svm/plot_weighted_samples.html#sphx-glr-auto-examples-svm-plot-weighted-samples-py"><span class="std std-ref">SVM: Weighted samples</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM: Weighted samples</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The following example illustrates the effect of scaling the regularization parameter when using svm for svm_classification. For SVC classification, we are interested in a risk minimization for the equation:"><img alt="" src="../_images/sphx_glr_plot_svm_scale_c_thumb.png" />
<p><a class="reference internal" href="svm/plot_svm_scale_c.html#sphx-glr-auto-examples-svm-plot-svm-scale-c-py"><span class="std std-ref">Scaling the regularization parameter for SVCs</span></a></p>
  <div class="sphx-glr-thumbnail-title">Scaling the regularization parameter for SVCs</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Toy example of 1D regression using linear, polynomial and RBF kernels."><img alt="" src="../_images/sphx_glr_plot_svm_regression_thumb.png" />
<p><a class="reference internal" href="svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py"><span class="std std-ref">Support Vector Regression (SVR) using linear and non-linear kernels</span></a></p>
  <div class="sphx-glr-thumbnail-title">Support Vector Regression (SVR) using linear and non-linear kernels</div>
</div></div></section>
<section id="tutorial-exercises">
<h2>Tutorial exercises<a class="headerlink" href="#tutorial-exercises" title="Link to this heading">#</a></h2>
<p>Exercises for the tutorials</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A tutorial exercise which uses cross-validation with linear models."><img alt="" src="../_images/sphx_glr_plot_cv_diabetes_thumb.png" />
<p><a class="reference internal" href="exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py"><span class="std std-ref">Cross-validation on diabetes Dataset Exercise</span></a></p>
  <div class="sphx-glr-thumbnail-title">Cross-validation on diabetes Dataset Exercise</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A tutorial exercise regarding the use of classification techniques on the Digits dataset."><img alt="" src="../_images/sphx_glr_plot_digits_classification_exercise_thumb.png" />
<p><a class="reference internal" href="exercises/plot_digits_classification_exercise.html#sphx-glr-auto-examples-exercises-plot-digits-classification-exercise-py"><span class="std std-ref">Digits Classification Exercise</span></a></p>
  <div class="sphx-glr-thumbnail-title">Digits Classification Exercise</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A tutorial exercise for using different SVM kernels."><img alt="" src="../_images/sphx_glr_plot_iris_exercise_thumb.png" />
<p><a class="reference internal" href="exercises/plot_iris_exercise.html#sphx-glr-auto-examples-exercises-plot-iris-exercise-py"><span class="std std-ref">SVM Exercise</span></a></p>
  <div class="sphx-glr-thumbnail-title">SVM Exercise</div>
</div></div></section>
<section id="working-with-text-documents">
<h2>Working with text documents<a class="headerlink" href="#working-with-text-documents" title="Link to this heading">#</a></h2>
<p>Examples concerning the <a class="reference internal" href="../api/sklearn.feature_extraction.html#module-sklearn.feature_extraction.text" title="sklearn.feature_extraction.text"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text</span></code></a> module.</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This is an example showing how scikit-learn can be used to classify documents by topics using a Bag of Words approach. This example uses a Tf-idf-weighted document-term sparse matrix to encode the features and demonstrates various classifiers that can efficiently handle sparse matrices."><img alt="" src="../_images/sphx_glr_plot_document_classification_20newsgroups_thumb.png" />
<p><a class="reference internal" href="text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></p>
  <div class="sphx-glr-thumbnail-title">Classification of text documents using sparse features</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This is an example showing how the scikit-learn API can be used to cluster documents by topics using a Bag of Words approach."><img alt="" src="../_images/sphx_glr_plot_document_clustering_thumb.png" />
<p><a class="reference internal" href="text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a></p>
  <div class="sphx-glr-thumbnail-title">Clustering text documents using k-means</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example we illustrate text vectorization, which is the process of representing non-numerical input data (such as dictionaries or text documents) as vectors of real numbers."><img alt="" src="../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_thumb.png" />
<p><a class="reference internal" href="text/plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"><span class="std std-ref">FeatureHasher and DictVectorizer Comparison</span></a></p>
  <div class="sphx-glr-thumbnail-title">FeatureHasher and DictVectorizer Comparison</div>
</div></div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-footer sphx-glr-footer-gallery docutils container">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/07fcc19ba03226cd3d83d4e40ec44385/auto_examples_python.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">auto_examples_python.zip</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6f1e7a639e0699d6164445b55e6c116d/auto_examples_jupyter.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Jupyter</span> <span class="pre">notebooks:</span> <span class="pre">auto_examples_jupyter.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="prev-next-area">
    <a class="left-prev"
       href="../modules/generated/sklearn.utils.register_parallel_backend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">register_parallel_backend</p>
      </div>
    </a>
    <a class="right-next"
       href="release_highlights/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Release Highlights</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>
                </footer>
              
              
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>