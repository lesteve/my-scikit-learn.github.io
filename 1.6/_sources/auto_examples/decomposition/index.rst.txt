

.. _sphx_glr_auto_examples_decomposition:

.. _decomposition_examples:

Decomposition
-------------

Examples concerning the :mod:`sklearn.decomposition` module.




.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="An example of estimating sources from noisy data.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_ica_blind_source_separation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_ica_blind_source_separation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Blind source separation using FastICA</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_pca_vs_lda_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparison of LDA and PCA 2D projection of Iris dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example applies to olivetti_faces_dataset different unsupervised matrix decomposition (dimension reduction) methods from the module sklearn.decomposition (see the documentation chapter decompositions).">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_faces_decomposition_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Faces dataset decompositions</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Investigating the Iris dataset, we see that sepal length, petal length and petal width are highly correlated. Sepal width is less redundant. Matrix decomposition techniques can uncover these latent patterns. Applying rotations to the resulting components does not inherently improve the predictive value of the derived latent space, but can help visualise their structure; here, for example, the varimax rotation, which is found by maximizing the squared variances of the weights, finds a structure where the second component only loads positively on sepal width.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_varimax_fa_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Factor Analysis (with rotation) to visualize patterns</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_ica_vs_pca_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_ica_vs_pca.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">FastICA on 2D point clouds</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="An example comparing the effect of reconstructing noisy fragments of a raccoon face image using firstly online DictionaryLearning and various transform methods.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_image_denoising_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_image_denoising.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Image denoising using dictionary learning</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_incremental_pca_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Incremental PCA</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example shows the difference between the Principal Components Analysis (~sklearn.decomposition.PCA) and its kernelized version (~sklearn.decomposition.KernelPCA).">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_kernel_pca_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Kernel PCA</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Probabilistic PCA and Factor Analysis are probabilistic models. The consequence is that the likelihood of new data can be used for model selection and covariance estimation. Here we compare PCA and FA with cross-validation on low rank data corrupted with homoscedastic noise (noise variance is the same for each feature) or heteroscedastic noise (noise variance is the different for each feature). In a second step we compare the model likelihood to the likelihoods obtained from shrinkage covariance estimators.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_pca_vs_fa_model_selection_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_fa_model_selection.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Model selection with Probabilistic PCA and Factor Analysis (FA)</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example shows a well known decomposition technique known as Principal Component Analysis (PCA) on the Iris dataset.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_pca_iris_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Principal Component Analysis (PCA) on Iris Dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the SparseCoder estimator. The Ricker (also known as Mexican hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates learning the dictionary to best fit your type of signals.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_sparse_coding_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_sparse_coding.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Sparse coding with a precomputed dictionary</div>
    </div>


.. thumbnail-parent-div-close

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/decomposition/plot_ica_blind_source_separation
   /auto_examples/decomposition/plot_pca_vs_lda
   /auto_examples/decomposition/plot_faces_decomposition
   /auto_examples/decomposition/plot_varimax_fa
   /auto_examples/decomposition/plot_ica_vs_pca
   /auto_examples/decomposition/plot_image_denoising
   /auto_examples/decomposition/plot_incremental_pca
   /auto_examples/decomposition/plot_kernel_pca
   /auto_examples/decomposition/plot_pca_vs_fa_model_selection
   /auto_examples/decomposition/plot_pca_iris
   /auto_examples/decomposition/plot_sparse_coding

